{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Houses Kaggle Competition (revisited with Deep Learning üî•) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/kaggle-batch-challenge.png' width=600>](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "‚öôÔ∏è Let's re-use our previous **pipeline** built in the module **`05-07-Ensemble-Methods`** and try to improve our final predictions with a Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# DATA MANIPULATION\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "# DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# VIEWING OPTIONS IN THE NOTEBOOK\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) üöÄ Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Load the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíæ Let's load our **training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv\")\n",
    "X = data.drop(columns='SalePrice')\n",
    "y = data['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>706</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>8</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>978</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>486</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>216</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>655</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig LandSlope Neighborhood Condition1  \\\n",
       "0         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "1         Lvl    AllPub       FR2       Gtl      Veenker      Feedr   \n",
       "2         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "3         Lvl    AllPub    Corner       Gtl      Crawfor       Norm   \n",
       "4         Lvl    AllPub       FR2       Gtl      NoRidge       Norm   \n",
       "\n",
       "  Condition2 BldgType HouseStyle  OverallQual  OverallCond  YearBuilt  \\\n",
       "0       Norm     1Fam     2Story            7            5       2003   \n",
       "1       Norm     1Fam     1Story            6            8       1976   \n",
       "2       Norm     1Fam     2Story            7            5       2001   \n",
       "3       Norm     1Fam     2Story            7            5       1915   \n",
       "4       Norm     1Fam     2Story            8            5       2000   \n",
       "\n",
       "   YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType  \\\n",
       "0          2003     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "1          1976     Gable  CompShg     MetalSd     MetalSd       None   \n",
       "2          2002     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "3          1970     Gable  CompShg     Wd Sdng     Wd Shng       None   \n",
       "4          2000     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "\n",
       "   MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure  \\\n",
       "0       196.0        Gd        TA      PConc       Gd       TA           No   \n",
       "1         0.0        TA        TA     CBlock       Gd       TA           Gd   \n",
       "2       162.0        Gd        TA      PConc       Gd       TA           Mn   \n",
       "3         0.0        TA        TA     BrkTil       TA       Gd           No   \n",
       "4       350.0        Gd        TA      PConc       Gd       TA           Av   \n",
       "\n",
       "  BsmtFinType1  BsmtFinSF1 BsmtFinType2  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \\\n",
       "0          GLQ         706          Unf           0        150          856   \n",
       "1          ALQ         978          Unf           0        284         1262   \n",
       "2          GLQ         486          Unf           0        434          920   \n",
       "3          ALQ         216          Unf           0        540          756   \n",
       "4          GLQ         655          Unf           0        490         1145   \n",
       "\n",
       "  Heating HeatingQC CentralAir Electrical  1stFlrSF  2ndFlrSF  LowQualFinSF  \\\n",
       "0    GasA        Ex          Y      SBrkr       856       854             0   \n",
       "1    GasA        Ex          Y      SBrkr      1262         0             0   \n",
       "2    GasA        Ex          Y      SBrkr       920       866             0   \n",
       "3    GasA        Gd          Y      SBrkr       961       756             0   \n",
       "4    GasA        Ex          Y      SBrkr      1145      1053             0   \n",
       "\n",
       "   GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  \\\n",
       "0       1710             1             0         2         1             3   \n",
       "1       1262             0             1         2         0             3   \n",
       "2       1786             1             0         2         1             3   \n",
       "3       1717             1             0         1         0             3   \n",
       "4       2198             1             0         2         1             4   \n",
       "\n",
       "   KitchenAbvGr KitchenQual  TotRmsAbvGrd Functional  Fireplaces FireplaceQu  \\\n",
       "0             1          Gd             8        Typ           0         NaN   \n",
       "1             1          TA             6        Typ           1          TA   \n",
       "2             1          Gd             6        Typ           1          TA   \n",
       "3             1          Gd             7        Typ           1          Gd   \n",
       "4             1          Gd             9        Typ           1          TA   \n",
       "\n",
       "  GarageType  GarageYrBlt GarageFinish  GarageCars  GarageArea GarageQual  \\\n",
       "0     Attchd       2003.0          RFn           2         548         TA   \n",
       "1     Attchd       1976.0          RFn           2         460         TA   \n",
       "2     Attchd       2001.0          RFn           2         608         TA   \n",
       "3     Detchd       1998.0          Unf           3         642         TA   \n",
       "4     Attchd       2000.0          RFn           3         836         TA   \n",
       "\n",
       "  GarageCond PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  \\\n",
       "0         TA          Y           0           61              0          0   \n",
       "1         TA          Y         298            0              0          0   \n",
       "2         TA          Y           0           42              0          0   \n",
       "3         TA          Y           0           35            272          0   \n",
       "4         TA          Y         192           84              0          0   \n",
       "\n",
       "   ScreenPorch  PoolArea PoolQC Fence MiscFeature  MiscVal  MoSold  YrSold  \\\n",
       "0            0         0    NaN   NaN         NaN        0       2    2008   \n",
       "1            0         0    NaN   NaN         NaN        0       5    2007   \n",
       "2            0         0    NaN   NaN         NaN        0       9    2008   \n",
       "3            0         0    NaN   NaN         NaN        0       2    2006   \n",
       "4            0         0    NaN   NaN         NaN        0      12    2008   \n",
       "\n",
       "  SaleType SaleCondition  \n",
       "0       WD        Normal  \n",
       "1       WD        Normal  \n",
       "2       WD        Normal  \n",
       "3       WD       Abnorml  \n",
       "4       WD        Normal  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 80), (1460,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([208500, 181500, 223500, ..., 266500, 142125, 147500])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíæ Let's also load the **test set**\n",
    "\n",
    "‚ùóÔ∏è Remember ‚ùóÔ∏è You have access to `X_test` but only Kaggle has `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Train/Val Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Holdout** ‚ùì \n",
    "\n",
    "As you are not allowed to use the test set (and you don't have access to `y_test` anyway), split your dataset into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val= train_test_split(X, y, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Import the preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ You will find in `utils/preprocessor.py` the **`data-preprocessing pipeline`** that was built in our previous iteration.\n",
    "\n",
    "‚ùì Run the cell below, and make sure you understand what the pipeline does. Look at the code in `preprocessor.py` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numerical_encoder&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;knnimputer&#x27;,\n",
       "                                                                   KNNImputer()),\n",
       "                                                                  (&#x27;minmaxscaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;3SsnPorch&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;,\n",
       "                                                   &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;GarageCars...\n",
       "                                                   &#x27;CentralAir&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;,\n",
       "                                                   &#x27;GarageType&#x27;, &#x27;Heating&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                                   &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;,\n",
       "                                                   &#x27;MiscFeature&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;,\n",
       "                                                   &#x27;RoofStyle&#x27;, &#x27;SaleCondition&#x27;,\n",
       "                                                   &#x27;SaleType&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Utilities&#x27;])])),\n",
       "                (&#x27;selectpercentile&#x27;,\n",
       "                 SelectPercentile(percentile=75,\n",
       "                                  score_func=&lt;function mutual_info_regression at 0x7f4362127760&gt;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numerical_encoder&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;knnimputer&#x27;,\n",
       "                                                                   KNNImputer()),\n",
       "                                                                  (&#x27;minmaxscaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;3SsnPorch&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;,\n",
       "                                                   &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;GarageCars...\n",
       "                                                   &#x27;CentralAir&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;,\n",
       "                                                   &#x27;GarageType&#x27;, &#x27;Heating&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                                   &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;,\n",
       "                                                   &#x27;MiscFeature&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;,\n",
       "                                                   &#x27;RoofStyle&#x27;, &#x27;SaleCondition&#x27;,\n",
       "                                                   &#x27;SaleType&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Utilities&#x27;])])),\n",
       "                (&#x27;selectpercentile&#x27;,\n",
       "                 SelectPercentile(percentile=75,\n",
       "                                  score_func=&lt;function mutual_info_regression at 0x7f4362127760&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;numerical_encoder&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;knnimputer&#x27;, KNNImputer()),\n",
       "                                                 (&#x27;minmaxscaler&#x27;,\n",
       "                                                  MinMaxScaler())]),\n",
       "                                 [&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;3SsnPorch&#x27;,\n",
       "                                  &#x27;BedroomAbvGr&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;,\n",
       "                                  &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;,\n",
       "                                  &#x27;EnclosedPorch&#x27;, &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;,\n",
       "                                  &#x27;GarageArea&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageYrBlt&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;HalfBath...\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;onehotencoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Alley&#x27;, &#x27;BldgType&#x27;, &#x27;CentralAir&#x27;,\n",
       "                                  &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;, &#x27;GarageType&#x27;,\n",
       "                                  &#x27;Heating&#x27;, &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;, &#x27;MiscFeature&#x27;,\n",
       "                                  &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                  &#x27;SaleCondition&#x27;, &#x27;SaleType&#x27;, &#x27;Street&#x27;,\n",
       "                                  &#x27;Utilities&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;3SsnPorch&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;, &#x27;GarageArea&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GrLivArea&#x27;, &#x27;HalfBath&#x27;, &#x27;Id&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;LotArea&#x27;, &#x27;LotFrontage&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;MSSubClass&#x27;, &#x27;MasVnrArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;OverallCond&#x27;, &#x27;OverallQual&#x27;, &#x27;PoolArea&#x27;, &#x27;ScreenPorch&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;YrSold&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ordinal_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;BsmtQual&#x27;, &#x27;Electrical&#x27;, &#x27;ExterCond&#x27;, &#x27;ExterQual&#x27;, &#x27;Fence&#x27;, &#x27;FireplaceQu&#x27;, &#x27;Functional&#x27;, &#x27;GarageCond&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;HeatingQC&#x27;, &#x27;KitchenQual&#x27;, &#x27;LandContour&#x27;, &#x27;LandSlope&#x27;, &#x27;LotShape&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;missing&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder(categories=[[&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;No&#x27;, &#x27;Mn&#x27;, &#x27;Av&#x27;, &#x27;Gd&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Unf&#x27;, &#x27;LwQ&#x27;, &#x27;Rec&#x27;, &#x27;BLQ&#x27;, &#x27;ALQ&#x27;,\n",
       "                            &#x27;GLQ&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Unf&#x27;, &#x27;LwQ&#x27;, &#x27;Rec&#x27;, &#x27;BLQ&#x27;, &#x27;ALQ&#x27;,\n",
       "                            &#x27;GLQ&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Mix&#x27;, &#x27;FuseP&#x27;, &#x27;FuseF&#x27;, &#x27;FuseA&#x27;,\n",
       "                            &#x27;SBrkr&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;...\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Unf&#x27;, &#x27;RFn&#x27;, &#x27;Fin&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Low&#x27;, &#x27;Bnk&#x27;, &#x27;HLS&#x27;, &#x27;Lvl&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Sev&#x27;, &#x27;Mod&#x27;, &#x27;Gtl&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;IR3&#x27;, &#x27;IR2&#x27;, &#x27;IR1&#x27;, &#x27;Reg&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;N&#x27;, &#x27;P&#x27;, &#x27;Y&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;]],\n",
       "               handle_unknown=&#x27;use_encoded_value&#x27;, unknown_value=-1)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">nominal_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Alley&#x27;, &#x27;BldgType&#x27;, &#x27;CentralAir&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;, &#x27;GarageType&#x27;, &#x27;Heating&#x27;, &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;, &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;, &#x27;MiscFeature&#x27;, &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;, &#x27;RoofStyle&#x27;, &#x27;SaleCondition&#x27;, &#x27;SaleType&#x27;, &#x27;Street&#x27;, &#x27;Utilities&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectPercentile</label><div class=\"sk-toggleable__content\"><pre>SelectPercentile(percentile=75,\n",
       "                 score_func=&lt;function mutual_info_regression at 0x7f4362127760&gt;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('numerical_encoder',\n",
       "                                                  Pipeline(steps=[('knnimputer',\n",
       "                                                                   KNNImputer()),\n",
       "                                                                  ('minmaxscaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['1stFlrSF', '2ndFlrSF',\n",
       "                                                   '3SsnPorch', 'BedroomAbvGr',\n",
       "                                                   'BsmtFinSF1', 'BsmtFinSF2',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'BsmtUnfSF',\n",
       "                                                   'EnclosedPorch',\n",
       "                                                   'Fireplaces', 'FullBath',\n",
       "                                                   'GarageArea', 'GarageCars...\n",
       "                                                   'CentralAir', 'Condition1',\n",
       "                                                   'Condition2', 'Exterior1st',\n",
       "                                                   'Exterior2nd', 'Foundation',\n",
       "                                                   'GarageType', 'Heating',\n",
       "                                                   'HouseStyle', 'LotConfig',\n",
       "                                                   'MSZoning', 'MasVnrType',\n",
       "                                                   'MiscFeature',\n",
       "                                                   'Neighborhood', 'RoofMatl',\n",
       "                                                   'RoofStyle', 'SaleCondition',\n",
       "                                                   'SaleType', 'Street',\n",
       "                                                   'Utilities'])])),\n",
       "                ('selectpercentile',\n",
       "                 SelectPercentile(percentile=75,\n",
       "                                  score_func=<function mutual_info_regression at 0x7f4362127760>))])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.preprocessor import create_preproc\n",
    "\n",
    "preproc = create_preproc(X_train)\n",
    "preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Scaling your numerical features and encoding the categorical features** ‚ùì\n",
    "\n",
    "Apply these transformations to _both_ your training set and your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "preproc.fit(X_train, y_train)\n",
    "X_train_scaled= preproc.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 159)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled= preproc.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 159)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) üîÆ Your predictions in Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ This is your first **regression** task with Keras! \n",
    "\n",
    "üí° Here a few tips to get started:\n",
    "- Kaggle's [rule](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) requires to minimize **`rmsle`** (Root Mean Square Log Error). \n",
    "    - As you can see, we can specify `msle` directly as a loss-function with Tensorflow.Keras!\n",
    "    - Just remember to take the square-root of your loss results to read your rmsle metric.\n",
    "    \n",
    "    \n",
    "üòÉ The best boosted-tree ***rmsle*** score to beat is around ***0.13***\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://i.pinimg.com/564x/4c/fe/ef/4cfeef34af09973211f584e8307b433c.jpg\" alt=\"`Impossible mission\" style=\"height: 300px; width:500px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "‚ùì **Your mission, should you choose to accept it:** ‚ùì\n",
    "- üí™ Beat the best boosted-tree üí™ \n",
    "\n",
    "    - Your responsibilities are:\n",
    "        - to build the ***best neural network architecture*** possible,\n",
    "        - and to control the number of epochs to ***avoid overfitting***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Predicting the houses' prices using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Preliminary Question: Initializing a Neural Network** ‚ùì\n",
    "\n",
    "Create a function `initialize_model` which initializes a Dense Neural network:\n",
    "- You are responsible for designing the architecture (number of layers, number of neurons)\n",
    "- The function should also compile the model with the following parameters:\n",
    "    - ***optimizer = \"adam\"***\n",
    "    - ***loss = \"msle\"*** (_Optimizing directly for the Squared Log Error!_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_model(input_dim):\n",
    "    model= Sequential()\n",
    "    model.add(layers.Dense(20, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "#     model.add(layers.Dense(3, activation='relu'))\n",
    "#     model.add(layers.Dense(2, activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='msle',\n",
    "                  optimizer='adam') \n",
    "#                   metrics=[rmsle])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= initialize_model(input_dim=159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_338 (Dense)           (None, 20)                3200      \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 10)                210       \n",
      "                                                                 \n",
      " dense_340 (Dense)           (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_341 (Dense)           (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,471\n",
      "Trainable params: 3,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions/Guidance** ‚ùì\n",
    "\n",
    "1. Initialize a Neural Network\n",
    "2. Train it\n",
    "3. Evaluate its performance\n",
    "4. Is the model overfitting the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "103/103 [==============================] - 1s 3ms/step - loss: 129.0532 - val_loss: 110.8630\n",
      "Epoch 2/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 97.0703 - val_loss: 85.4017\n",
      "Epoch 3/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 78.1304 - val_loss: 71.3513\n",
      "Epoch 4/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 66.6426 - val_loss: 61.9559\n",
      "Epoch 5/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 58.5483 - val_loss: 55.0006\n",
      "Epoch 6/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 52.3691 - val_loss: 49.5308\n",
      "Epoch 7/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 47.4127 - val_loss: 45.0591\n",
      "Epoch 8/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 43.3075 - val_loss: 41.3063\n",
      "Epoch 9/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 39.8229 - val_loss: 38.0880\n",
      "Epoch 10/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 36.8127 - val_loss: 35.2859\n",
      "Epoch 11/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 34.1747 - val_loss: 32.8132\n",
      "Epoch 12/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 31.8369 - val_loss: 30.6143\n",
      "Epoch 13/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 29.7449 - val_loss: 28.6359\n",
      "Epoch 14/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 27.8576 - val_loss: 26.8448\n",
      "Epoch 15/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 26.1443 - val_loss: 25.2142\n",
      "Epoch 16/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 24.5802 - val_loss: 23.7210\n",
      "Epoch 17/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 23.1443 - val_loss: 22.3468\n",
      "Epoch 18/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 21.8189 - val_loss: 21.0770\n",
      "Epoch 19/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 20.5933 - val_loss: 19.8996\n",
      "Epoch 20/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 19.4548 - val_loss: 18.8047\n",
      "Epoch 21/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 18.3945 - val_loss: 17.7838\n",
      "Epoch 22/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 17.4039 - val_loss: 16.8273\n",
      "Epoch 23/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 16.4760 - val_loss: 15.9327\n",
      "Epoch 24/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 15.6052 - val_loss: 15.0911\n",
      "Epoch 25/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 14.7861 - val_loss: 14.2981\n",
      "Epoch 26/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 14.0146 - val_loss: 13.5519\n",
      "Epoch 27/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 13.2868 - val_loss: 12.8470\n",
      "Epoch 28/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 12.5989 - val_loss: 12.1799\n",
      "Epoch 29/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 11.9477 - val_loss: 11.5487\n",
      "Epoch 30/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 11.3314 - val_loss: 10.9507\n",
      "Epoch 31/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 10.7466 - val_loss: 10.3830\n",
      "Epoch 32/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 10.1920 - val_loss: 9.8446\n",
      "Epoch 33/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 9.6647 - val_loss: 9.3322\n",
      "Epoch 34/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 9.1638 - val_loss: 8.8455\n",
      "Epoch 35/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 8.6871 - val_loss: 8.3825\n",
      "Epoch 36/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 8.2334 - val_loss: 7.9418\n",
      "Epoch 37/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 7.8015 - val_loss: 7.5215\n",
      "Epoch 38/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 7.3897 - val_loss: 7.1216\n",
      "Epoch 39/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 6.9973 - val_loss: 6.7398\n",
      "Epoch 40/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 6.6231 - val_loss: 6.3759\n",
      "Epoch 41/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 6.2668 - val_loss: 6.0297\n",
      "Epoch 42/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 5.9274 - val_loss: 5.6995\n",
      "Epoch 43/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 5.6027 - val_loss: 5.3837\n",
      "Epoch 44/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 5.2932 - val_loss: 5.0830\n",
      "Epoch 45/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 4.9980 - val_loss: 4.7962\n",
      "Epoch 46/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 4.7165 - val_loss: 4.5229\n",
      "Epoch 47/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 4.4481 - val_loss: 4.2621\n",
      "Epoch 48/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 4.1923 - val_loss: 4.0136\n",
      "Epoch 49/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9486 - val_loss: 3.7769\n",
      "Epoch 50/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7164 - val_loss: 3.5512\n",
      "Epoch 51/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4951 - val_loss: 3.3370\n",
      "Epoch 52/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2842 - val_loss: 3.1321\n",
      "Epoch 53/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0835 - val_loss: 2.9377\n",
      "Epoch 54/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8926 - val_loss: 2.7526\n",
      "Epoch 55/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.7109 - val_loss: 2.5769\n",
      "Epoch 56/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.5384 - val_loss: 2.4096\n",
      "Epoch 57/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.3748 - val_loss: 2.2516\n",
      "Epoch 58/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.2194 - val_loss: 2.1011\n",
      "Epoch 59/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.0720 - val_loss: 1.9589\n",
      "Epoch 60/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.9321 - val_loss: 1.8235\n",
      "Epoch 61/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.8000 - val_loss: 1.6961\n",
      "Epoch 62/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.6751 - val_loss: 1.5759\n",
      "Epoch 63/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.5571 - val_loss: 1.4624\n",
      "Epoch 64/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.4456 - val_loss: 1.3551\n",
      "Epoch 65/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.3408 - val_loss: 1.2546\n",
      "Epoch 66/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.2418 - val_loss: 1.1592\n",
      "Epoch 67/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.1487 - val_loss: 1.0698\n",
      "Epoch 68/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0615 - val_loss: 0.9863\n",
      "Epoch 69/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9796 - val_loss: 0.9082\n",
      "Epoch 70/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9031 - val_loss: 0.8348\n",
      "Epoch 71/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.8316 - val_loss: 0.7667\n",
      "Epoch 72/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.7647 - val_loss: 0.7030\n",
      "Epoch 73/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.7025 - val_loss: 0.6435\n",
      "Epoch 74/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 0.5883\n",
      "Epoch 75/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.5377\n",
      "Epoch 76/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.5411 - val_loss: 0.4909\n",
      "Epoch 77/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.4954 - val_loss: 0.4476\n",
      "Epoch 78/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.4532 - val_loss: 0.4078\n",
      "Epoch 79/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.3712\n",
      "Epoch 80/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.3790 - val_loss: 0.3381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.3467 - val_loss: 0.3080\n",
      "Epoch 82/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.3173 - val_loss: 0.2807\n",
      "Epoch 83/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2905 - val_loss: 0.2556\n",
      "Epoch 84/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2664 - val_loss: 0.2336\n",
      "Epoch 85/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2447 - val_loss: 0.2133\n",
      "Epoch 86/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2253 - val_loss: 0.1955\n",
      "Epoch 87/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2078 - val_loss: 0.1797\n",
      "Epoch 88/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1923 - val_loss: 0.1655\n",
      "Epoch 89/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1785 - val_loss: 0.1530\n",
      "Epoch 90/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 0.1421\n",
      "Epoch 91/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 0.1328\n",
      "Epoch 92/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1467 - val_loss: 0.1244\n",
      "Epoch 93/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.1174\n",
      "Epoch 94/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.1113\n",
      "Epoch 95/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1259 - val_loss: 0.1063\n",
      "Epoch 96/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.1020\n",
      "Epoch 97/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.0984\n",
      "Epoch 98/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0954\n",
      "Epoch 99/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.0928\n",
      "Epoch 100/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1074 - val_loss: 0.0907\n",
      "Epoch 101/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1053 - val_loss: 0.0891\n",
      "Epoch 102/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.0876\n",
      "Epoch 103/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.0865\n",
      "Epoch 104/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.0855\n",
      "Epoch 105/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0998 - val_loss: 0.0847\n",
      "Epoch 106/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0839\n",
      "Epoch 107/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0980 - val_loss: 0.0833\n",
      "Epoch 108/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.0827\n",
      "Epoch 109/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.0822\n",
      "Epoch 110/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0960 - val_loss: 0.0817\n",
      "Epoch 111/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0954 - val_loss: 0.0811\n",
      "Epoch 112/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0948 - val_loss: 0.0806\n",
      "Epoch 113/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.0801\n",
      "Epoch 114/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0937 - val_loss: 0.0796\n",
      "Epoch 115/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0790\n",
      "Epoch 116/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.0785\n",
      "Epoch 117/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0918 - val_loss: 0.0779\n",
      "Epoch 118/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0912 - val_loss: 0.0773\n",
      "Epoch 119/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0905 - val_loss: 0.0767\n",
      "Epoch 120/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0899 - val_loss: 0.0760\n",
      "Epoch 121/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0892 - val_loss: 0.0754\n",
      "Epoch 122/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0885 - val_loss: 0.0747\n",
      "Epoch 123/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0740\n",
      "Epoch 124/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0870 - val_loss: 0.0733\n",
      "Epoch 125/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.0726\n",
      "Epoch 126/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0855 - val_loss: 0.0719\n",
      "Epoch 127/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0847 - val_loss: 0.0711\n",
      "Epoch 128/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0839 - val_loss: 0.0704\n",
      "Epoch 129/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.0696\n",
      "Epoch 130/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0823 - val_loss: 0.0688\n",
      "Epoch 131/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.0680\n",
      "Epoch 132/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0806 - val_loss: 0.0672\n",
      "Epoch 133/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0797 - val_loss: 0.0664\n",
      "Epoch 134/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.0655\n",
      "Epoch 135/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.0647\n",
      "Epoch 136/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.0639\n",
      "Epoch 137/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0762 - val_loss: 0.0630\n",
      "Epoch 138/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0753 - val_loss: 0.0621\n",
      "Epoch 139/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0744 - val_loss: 0.0613\n",
      "Epoch 140/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0734 - val_loss: 0.0604\n",
      "Epoch 141/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0726 - val_loss: 0.0596\n",
      "Epoch 142/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0716 - val_loss: 0.0587\n",
      "Epoch 143/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0707 - val_loss: 0.0579\n",
      "Epoch 144/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.0570\n",
      "Epoch 145/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.0562\n",
      "Epoch 146/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0679 - val_loss: 0.0553\n",
      "Epoch 147/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0670 - val_loss: 0.0545\n",
      "Epoch 148/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0661 - val_loss: 0.0536\n",
      "Epoch 149/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0652 - val_loss: 0.0528\n",
      "Epoch 150/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0643 - val_loss: 0.0519\n",
      "Epoch 151/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0634 - val_loss: 0.0511\n",
      "Epoch 152/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0624 - val_loss: 0.0503\n",
      "Epoch 153/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.0495\n",
      "Epoch 154/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0486\n",
      "Epoch 155/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0598 - val_loss: 0.0478\n",
      "Epoch 156/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0589 - val_loss: 0.0469\n",
      "Epoch 157/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0460\n",
      "Epoch 158/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0452\n",
      "Epoch 159/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0443\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0435\n",
      "Epoch 161/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0541 - val_loss: 0.0426\n",
      "Epoch 162/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0418\n",
      "Epoch 163/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0522 - val_loss: 0.0410\n",
      "Epoch 164/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0402\n",
      "Epoch 165/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0394\n",
      "Epoch 166/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0387\n",
      "Epoch 167/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.0379\n",
      "Epoch 168/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0373\n",
      "Epoch 169/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0470 - val_loss: 0.0366\n",
      "Epoch 170/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0360\n",
      "Epoch 171/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.0353\n",
      "Epoch 172/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0448 - val_loss: 0.0348\n",
      "Epoch 173/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0342\n",
      "Epoch 174/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0337\n",
      "Epoch 175/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0331\n",
      "Epoch 176/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.0327\n",
      "Epoch 177/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.0321\n",
      "Epoch 178/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0317\n",
      "Epoch 179/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0403 - val_loss: 0.0313\n",
      "Epoch 180/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.0309\n",
      "Epoch 181/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0305\n",
      "Epoch 182/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.0301\n",
      "Epoch 183/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0382 - val_loss: 0.0298\n",
      "Epoch 184/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0294\n",
      "Epoch 185/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0291\n",
      "Epoch 186/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0288\n",
      "Epoch 187/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0285\n",
      "Epoch 188/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0283\n",
      "Epoch 189/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0280\n",
      "Epoch 190/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0277\n",
      "Epoch 191/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0275\n",
      "Epoch 192/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0273\n",
      "Epoch 193/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 0.0270\n",
      "Epoch 194/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0268\n",
      "Epoch 195/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0267\n",
      "Epoch 196/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0265\n",
      "Epoch 197/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.0263\n",
      "Epoch 198/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0261\n",
      "Epoch 199/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0259\n",
      "Epoch 200/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0258\n",
      "Epoch 201/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0257\n",
      "Epoch 202/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0255\n",
      "Epoch 203/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0253\n",
      "Epoch 204/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0252\n",
      "Epoch 205/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0251\n",
      "Epoch 206/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0250\n",
      "Epoch 207/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0249\n",
      "Epoch 208/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0248\n",
      "Epoch 209/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0247\n",
      "Epoch 210/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0246\n",
      "Epoch 211/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0244\n",
      "Epoch 212/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0243\n",
      "Epoch 213/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0243\n",
      "Epoch 214/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0242\n",
      "Epoch 215/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0241\n",
      "Epoch 216/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0240\n",
      "Epoch 217/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0239\n",
      "Epoch 218/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0238\n",
      "Epoch 219/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0238\n",
      "Epoch 220/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0237\n",
      "Epoch 221/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0237\n",
      "Epoch 222/300\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0236\n",
      "Epoch 223/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0235\n",
      "Epoch 224/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0234\n",
      "Epoch 225/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0234\n",
      "Epoch 226/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0234\n",
      "Epoch 227/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0233\n",
      "Epoch 228/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0232\n",
      "Epoch 229/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0232\n",
      "Epoch 230/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0231\n",
      "Epoch 231/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0231\n",
      "Epoch 232/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0230\n",
      "Epoch 233/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0230\n",
      "Epoch 234/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0229\n",
      "Epoch 235/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0229\n",
      "Epoch 236/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0229\n",
      "Epoch 237/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0229\n",
      "Epoch 238/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0228\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0227\n",
      "Epoch 240/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0227\n",
      "Epoch 241/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0226\n",
      "Epoch 242/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0226\n",
      "Epoch 243/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0226\n",
      "Epoch 244/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0226\n",
      "Epoch 245/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0225\n",
      "Epoch 246/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0225\n",
      "Epoch 247/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0224\n",
      "Epoch 248/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0225\n",
      "Epoch 249/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0223\n",
      "Epoch 250/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0223\n",
      "Epoch 251/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0223\n",
      "Epoch 252/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0222\n",
      "Epoch 253/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0223\n",
      "Epoch 254/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0222\n",
      "Epoch 255/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0222\n",
      "Epoch 256/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0221\n",
      "Epoch 257/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0221\n",
      "Epoch 258/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0220\n",
      "Epoch 259/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0221\n",
      "Epoch 260/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0220\n",
      "Epoch 261/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0220\n",
      "Epoch 262/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0219\n",
      "Epoch 263/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0219\n",
      "Epoch 264/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0219\n",
      "Epoch 265/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0218\n",
      "Epoch 266/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0218\n",
      "Epoch 267/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0218\n",
      "Epoch 268/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0218\n",
      "Epoch 269/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0218\n",
      "Epoch 270/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0217\n",
      "Epoch 271/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0217\n",
      "Epoch 272/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0217\n",
      "Epoch 273/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0216\n",
      "Epoch 274/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0217 - val_loss: 0.0216\n",
      "Epoch 275/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0216\n",
      "Epoch 276/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0216\n",
      "Epoch 277/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0215\n",
      "Epoch 278/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0215\n",
      "Epoch 279/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0215\n",
      "Epoch 280/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0215\n",
      "Epoch 281/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0215\n",
      "Epoch 282/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0214\n",
      "Epoch 283/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0214\n",
      "Epoch 284/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0213\n",
      "Epoch 285/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 286/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0214\n",
      "Epoch 287/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 288/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0213\n",
      "Epoch 289/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0213\n",
      "Epoch 290/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0212\n",
      "Epoch 291/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0212\n",
      "Epoch 292/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0212\n",
      "Epoch 293/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0211\n",
      "Epoch 294/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0211\n",
      "Epoch 295/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0211\n",
      "Epoch 296/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0211\n",
      "Epoch 297/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0204 - val_loss: 0.0211\n",
      "Epoch 298/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0202 - val_loss: 0.0211\n",
      "Epoch 299/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0211\n",
      "Epoch 300/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0210\n"
     ]
    }
   ],
   "source": [
    "history= model.fit(X_train_scaled, y_train, batch_size=10, epochs=300, validation_data=(X_val_scaled,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14486008686742083"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val_scaled, y_val)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ We coded a `plot_history` function that you can use to detect overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyy.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(np.sqrt(history.history['loss']))\n",
    "    plt.plot(np.sqrt(history.history['val_loss']))\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('RMSLE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Challenging yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Questions to challenge yourself:**\n",
    "- Are you satisfied with your score?\n",
    "- Before publishing it, ask yourself whether you could really trust it or not?\n",
    "- Have you cross-validated your neural network? \n",
    "    - Feel free to cross-validate it manually with a *for loop* in Python to make sure that your results are robust against the randomness of a _train-val split_ before before submitting to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function `evaluate_model` following the framework below üëá then use a for loop with `KFold` to manually cross validate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y):\n",
    "    rmsle_score= []\n",
    "    min_rmsle=[]\n",
    "    # Slicing the training set and the validation set\n",
    "    kf= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, valid_index in kf.split(X):\n",
    "\n",
    "        X_train= X.iloc[train_index].copy()\n",
    "        X_valid= X.iloc[valid_index].copy()\n",
    "        y_train= y.iloc[train_index].copy()\n",
    "        y_valid= y.iloc[valid_index].copy()\n",
    "    # Preprocessing \n",
    "        preproc.fit(X_train,y_train)\n",
    "        X_train_scaled= preproc.transform(X_train)\n",
    "        X_valid_scaled= preproc.transform(X_valid)\n",
    "    \n",
    "    # Training the model on the preprocessed training dataset\n",
    "        model= initialize_model(input_dim=X_train_scaled.shape[1])\n",
    "        history= model.fit(X_train_scaled, y_train, batch_size=10, epochs=300, validation_data=(X_valid_scaled, y_valid))\n",
    "    # Evaluating the model on the preprocessed validation dataset\n",
    "        msle_score= model.evaluate(X_valid_scaled, y_valid)\n",
    "        rmsle_score.append(msle_score**0.5)\n",
    "        min_rmsle.append(np.sqrt(min(history.history['loss'])))\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame({\n",
    "                'rmsle_final_epoch': rmsle_score,\n",
    "                'rmsle_min': min_rmsle\n",
    "                        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "117/117 [==============================] - 1s 3ms/step - loss: 120.3913 - val_loss: 96.0336\n",
      "Epoch 2/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 81.8884 - val_loss: 69.3258\n",
      "Epoch 3/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 61.3753 - val_loss: 53.8996\n",
      "Epoch 4/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 48.9431 - val_loss: 43.9612\n",
      "Epoch 5/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 40.8326 - val_loss: 37.4067\n",
      "Epoch 6/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 35.2195 - val_loss: 32.6251\n",
      "Epoch 7/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 30.9796 - val_loss: 28.8957\n",
      "Epoch 8/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 27.6002 - val_loss: 25.8634\n",
      "Epoch 9/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 24.4773 - val_loss: 22.6192\n",
      "Epoch 10/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 21.4627 - val_loss: 19.9343\n",
      "Epoch 11/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 19.0363 - val_loss: 17.7726\n",
      "Epoch 12/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 17.0548 - val_loss: 15.9801\n",
      "Epoch 13/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 15.3931 - val_loss: 14.4591\n",
      "Epoch 14/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 13.9700 - val_loss: 13.1468\n",
      "Epoch 15/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 12.7319 - val_loss: 11.9976\n",
      "Epoch 16/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 11.6416 - val_loss: 10.9796\n",
      "Epoch 17/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 10.6715 - val_loss: 10.0703\n",
      "Epoch 18/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 9.8018 - val_loss: 9.2529\n",
      "Epoch 19/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 9.0170 - val_loss: 8.5131\n",
      "Epoch 20/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 8.3049 - val_loss: 7.8409\n",
      "Epoch 21/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 7.6560 - val_loss: 7.2266\n",
      "Epoch 22/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 7.0624 - val_loss: 6.6648\n",
      "Epoch 23/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 6.5178 - val_loss: 6.1477\n",
      "Epoch 24/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 6.0166 - val_loss: 5.6725\n",
      "Epoch 25/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 5.5543 - val_loss: 5.2340\n",
      "Epoch 26/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 5.1273 - val_loss: 4.8279\n",
      "Epoch 27/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.7320 - val_loss: 4.4525\n",
      "Epoch 28/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.3657 - val_loss: 4.1048\n",
      "Epoch 29/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.0260 - val_loss: 3.7818\n",
      "Epoch 30/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.7105 - val_loss: 3.4823\n",
      "Epoch 31/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.4174 - val_loss: 3.2042\n",
      "Epoch 32/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.1450 - val_loss: 2.9452\n",
      "Epoch 33/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 2.8918 - val_loss: 2.7050\n",
      "Epoch 34/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 2.6565 - val_loss: 2.4823\n",
      "Epoch 35/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 2.4378 - val_loss: 2.2754\n",
      "Epoch 36/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 2.2346 - val_loss: 2.0829\n",
      "Epoch 37/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 2.0457 - val_loss: 1.9047\n",
      "Epoch 38/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.8704 - val_loss: 1.7392\n",
      "Epoch 39/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.7078 - val_loss: 1.5860\n",
      "Epoch 40/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.5571 - val_loss: 1.4442\n",
      "Epoch 41/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.4177 - val_loss: 1.3135\n",
      "Epoch 42/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.2888 - val_loss: 1.1927\n",
      "Epoch 43/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1697 - val_loss: 1.0814\n",
      "Epoch 44/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.0600 - val_loss: 0.9788\n",
      "Epoch 45/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9590 - val_loss: 0.8849\n",
      "Epoch 46/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8663 - val_loss: 0.7989\n",
      "Epoch 47/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7813 - val_loss: 0.7202\n",
      "Epoch 48/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7036 - val_loss: 0.6484\n",
      "Epoch 49/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.6327 - val_loss: 0.5833\n",
      "Epoch 50/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.5683 - val_loss: 0.5244\n",
      "Epoch 51/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.5098 - val_loss: 0.4710\n",
      "Epoch 52/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.4570 - val_loss: 0.4230\n",
      "Epoch 53/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.4094 - val_loss: 0.3800\n",
      "Epoch 54/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3418\n",
      "Epoch 55/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.3284 - val_loss: 0.3077\n",
      "Epoch 56/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2944 - val_loss: 0.2777\n",
      "Epoch 57/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2643 - val_loss: 0.2512\n",
      "Epoch 58/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2377 - val_loss: 0.2280\n",
      "Epoch 59/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2144 - val_loss: 0.2077\n",
      "Epoch 60/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 0.1903\n",
      "Epoch 61/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1765 - val_loss: 0.1756\n",
      "Epoch 62/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1613 - val_loss: 0.1629\n",
      "Epoch 63/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1483 - val_loss: 0.1524\n",
      "Epoch 64/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.1433\n",
      "Epoch 65/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.1360\n",
      "Epoch 66/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1203 - val_loss: 0.1299\n",
      "Epoch 67/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1138 - val_loss: 0.1251\n",
      "Epoch 68/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1085 - val_loss: 0.1211\n",
      "Epoch 69/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.1181\n",
      "Epoch 70/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1007 - val_loss: 0.1156\n",
      "Epoch 71/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.1136\n",
      "Epoch 72/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.1122\n",
      "Epoch 73/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0937 - val_loss: 0.1110\n",
      "Epoch 74/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0922 - val_loss: 0.1100\n",
      "Epoch 75/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0910 - val_loss: 0.1093\n",
      "Epoch 76/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.1087\n",
      "Epoch 77/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0891 - val_loss: 0.1081\n",
      "Epoch 78/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.1076\n",
      "Epoch 79/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.1070\n",
      "Epoch 80/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.1065\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0864 - val_loss: 0.1059\n",
      "Epoch 82/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0858 - val_loss: 0.1053\n",
      "Epoch 83/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.1046\n",
      "Epoch 84/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.1039\n",
      "Epoch 85/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0840 - val_loss: 0.1032\n",
      "Epoch 86/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0833 - val_loss: 0.1024\n",
      "Epoch 87/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0826 - val_loss: 0.1017\n",
      "Epoch 88/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0819 - val_loss: 0.1008\n",
      "Epoch 89/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.1000\n",
      "Epoch 90/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0804 - val_loss: 0.0990\n",
      "Epoch 91/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0796 - val_loss: 0.0981\n",
      "Epoch 92/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.0971\n",
      "Epoch 93/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.0961\n",
      "Epoch 94/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0772 - val_loss: 0.0951\n",
      "Epoch 95/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0763 - val_loss: 0.0940\n",
      "Epoch 96/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0754 - val_loss: 0.0929\n",
      "Epoch 97/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0746 - val_loss: 0.0919\n",
      "Epoch 98/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.0907\n",
      "Epoch 99/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0726 - val_loss: 0.0895\n",
      "Epoch 100/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0717 - val_loss: 0.0884\n",
      "Epoch 101/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0707 - val_loss: 0.0872\n",
      "Epoch 102/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0697 - val_loss: 0.0860\n",
      "Epoch 103/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.0847\n",
      "Epoch 104/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.0834\n",
      "Epoch 105/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0667 - val_loss: 0.0822\n",
      "Epoch 106/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0656 - val_loss: 0.0809\n",
      "Epoch 107/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0646 - val_loss: 0.0797\n",
      "Epoch 108/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0635 - val_loss: 0.0783\n",
      "Epoch 109/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0625 - val_loss: 0.0770\n",
      "Epoch 110/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.0755\n",
      "Epoch 111/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0604 - val_loss: 0.0743\n",
      "Epoch 112/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.0730\n",
      "Epoch 113/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0583 - val_loss: 0.0718\n",
      "Epoch 114/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0573 - val_loss: 0.0703\n",
      "Epoch 115/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.0691\n",
      "Epoch 116/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0678\n",
      "Epoch 117/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0542 - val_loss: 0.0665\n",
      "Epoch 118/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0650\n",
      "Epoch 119/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0521 - val_loss: 0.0638\n",
      "Epoch 120/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0511 - val_loss: 0.0626\n",
      "Epoch 121/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0502 - val_loss: 0.0615\n",
      "Epoch 122/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.0603\n",
      "Epoch 123/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0589\n",
      "Epoch 124/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0577\n",
      "Epoch 125/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0567\n",
      "Epoch 126/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0555\n",
      "Epoch 127/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0545\n",
      "Epoch 128/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0533\n",
      "Epoch 129/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0523\n",
      "Epoch 130/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0512\n",
      "Epoch 131/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.0503\n",
      "Epoch 132/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.0494\n",
      "Epoch 133/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0484\n",
      "Epoch 134/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0475\n",
      "Epoch 135/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0470\n",
      "Epoch 136/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0461\n",
      "Epoch 137/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0452\n",
      "Epoch 138/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.0444\n",
      "Epoch 139/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0439\n",
      "Epoch 140/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.0432\n",
      "Epoch 141/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0425\n",
      "Epoch 142/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0419\n",
      "Epoch 143/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0413\n",
      "Epoch 144/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0408\n",
      "Epoch 145/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0404\n",
      "Epoch 146/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0397\n",
      "Epoch 147/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0395\n",
      "Epoch 148/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0392\n",
      "Epoch 149/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0383\n",
      "Epoch 150/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0381\n",
      "Epoch 151/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0376\n",
      "Epoch 152/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0372\n",
      "Epoch 153/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0368\n",
      "Epoch 154/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0365\n",
      "Epoch 155/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0363\n",
      "Epoch 156/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0359\n",
      "Epoch 157/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0357\n",
      "Epoch 158/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0354\n",
      "Epoch 159/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0351\n",
      "Epoch 160/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0348\n",
      "Epoch 162/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0343\n",
      "Epoch 163/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0342\n",
      "Epoch 164/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0339\n",
      "Epoch 165/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0336\n",
      "Epoch 166/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0335\n",
      "Epoch 167/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0333\n",
      "Epoch 168/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0332\n",
      "Epoch 169/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0331\n",
      "Epoch 170/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0331\n",
      "Epoch 171/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0327\n",
      "Epoch 172/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0325\n",
      "Epoch 173/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0323\n",
      "Epoch 174/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0321\n",
      "Epoch 175/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0320\n",
      "Epoch 176/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0320\n",
      "Epoch 177/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0317\n",
      "Epoch 178/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0315\n",
      "Epoch 179/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0315\n",
      "Epoch 180/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0313\n",
      "Epoch 181/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0313\n",
      "Epoch 182/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0311\n",
      "Epoch 183/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0309\n",
      "Epoch 184/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0308\n",
      "Epoch 185/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0306\n",
      "Epoch 186/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0306\n",
      "Epoch 187/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0305\n",
      "Epoch 188/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0304\n",
      "Epoch 189/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0302\n",
      "Epoch 190/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0302\n",
      "Epoch 191/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0300\n",
      "Epoch 192/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0299\n",
      "Epoch 193/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0298\n",
      "Epoch 194/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0298\n",
      "Epoch 195/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0295\n",
      "Epoch 196/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0295\n",
      "Epoch 197/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0294\n",
      "Epoch 198/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0293\n",
      "Epoch 199/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0292\n",
      "Epoch 200/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0291\n",
      "Epoch 201/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0289\n",
      "Epoch 202/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0289\n",
      "Epoch 203/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0288\n",
      "Epoch 204/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0287\n",
      "Epoch 205/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0286\n",
      "Epoch 206/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0285\n",
      "Epoch 207/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0284\n",
      "Epoch 208/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0283\n",
      "Epoch 209/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0282\n",
      "Epoch 210/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0281\n",
      "Epoch 211/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0280\n",
      "Epoch 212/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0280\n",
      "Epoch 213/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0278\n",
      "Epoch 214/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0277\n",
      "Epoch 215/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0276\n",
      "Epoch 216/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0274\n",
      "Epoch 217/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0274\n",
      "Epoch 218/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0273\n",
      "Epoch 219/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0271\n",
      "Epoch 220/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0271\n",
      "Epoch 221/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0270\n",
      "Epoch 222/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0269\n",
      "Epoch 223/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0268\n",
      "Epoch 224/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0267\n",
      "Epoch 225/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0267\n",
      "Epoch 226/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0268\n",
      "Epoch 227/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0265\n",
      "Epoch 228/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0265\n",
      "Epoch 229/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0264\n",
      "Epoch 230/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0262\n",
      "Epoch 231/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0262\n",
      "Epoch 232/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0261\n",
      "Epoch 233/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0261\n",
      "Epoch 234/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0260\n",
      "Epoch 235/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0259\n",
      "Epoch 236/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0258\n",
      "Epoch 237/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0256\n",
      "Epoch 238/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0257\n",
      "Epoch 239/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0255\n",
      "Epoch 240/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0254\n",
      "Epoch 241/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0253\n",
      "Epoch 242/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0252\n",
      "Epoch 243/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0252\n",
      "Epoch 244/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0252\n",
      "Epoch 245/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0250\n",
      "Epoch 246/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0249\n",
      "Epoch 247/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0250\n",
      "Epoch 248/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0248\n",
      "Epoch 249/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0248\n",
      "Epoch 250/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0248\n",
      "Epoch 251/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0246\n",
      "Epoch 252/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0245\n",
      "Epoch 253/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0245\n",
      "Epoch 254/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0244\n",
      "Epoch 255/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0245\n",
      "Epoch 256/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0243\n",
      "Epoch 257/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0244\n",
      "Epoch 258/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0243\n",
      "Epoch 259/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0242\n",
      "Epoch 260/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0241\n",
      "Epoch 261/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0241\n",
      "Epoch 262/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0240\n",
      "Epoch 263/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0239\n",
      "Epoch 264/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0239\n",
      "Epoch 265/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0241\n",
      "Epoch 266/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0237\n",
      "Epoch 267/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0237\n",
      "Epoch 268/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0237\n",
      "Epoch 269/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0236\n",
      "Epoch 270/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0236\n",
      "Epoch 271/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0234\n",
      "Epoch 272/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.0234\n",
      "Epoch 273/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0234\n",
      "Epoch 274/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0234\n",
      "Epoch 275/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0235\n",
      "Epoch 276/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0233\n",
      "Epoch 277/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 0.0232\n",
      "Epoch 278/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0231\n",
      "Epoch 279/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0231\n",
      "Epoch 280/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0231\n",
      "Epoch 281/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0230\n",
      "Epoch 282/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0229\n",
      "Epoch 283/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0229\n",
      "Epoch 284/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0228\n",
      "Epoch 285/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0228\n",
      "Epoch 286/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0228\n",
      "Epoch 287/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0227\n",
      "Epoch 288/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0227\n",
      "Epoch 289/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0226\n",
      "Epoch 290/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0226\n",
      "Epoch 291/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0228\n",
      "Epoch 292/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0225\n",
      "Epoch 293/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0225\n",
      "Epoch 294/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0224\n",
      "Epoch 295/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0224\n",
      "Epoch 296/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0224\n",
      "Epoch 297/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0223\n",
      "Epoch 298/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0223\n",
      "Epoch 299/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0223\n",
      "Epoch 300/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0222\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0222\n",
      "Epoch 1/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 115.8088 - val_loss: 93.7064\n",
      "Epoch 2/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 81.0452 - val_loss: 71.6141\n",
      "Epoch 3/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 64.0532 - val_loss: 58.0961\n",
      "Epoch 4/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 52.9737 - val_loss: 49.1335\n",
      "Epoch 5/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 45.4704 - val_loss: 42.7936\n",
      "Epoch 6/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 39.9601 - val_loss: 37.9611\n",
      "Epoch 7/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 35.6532 - val_loss: 34.0874\n",
      "Epoch 8/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 32.1440 - val_loss: 30.8792\n",
      "Epoch 9/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 29.2011 - val_loss: 28.1551\n",
      "Epoch 10/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 26.6806 - val_loss: 25.7998\n",
      "Epoch 11/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 24.4864 - val_loss: 23.7347\n",
      "Epoch 12/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 22.5519 - val_loss: 21.9031\n",
      "Epoch 13/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 20.8283 - val_loss: 20.2634\n",
      "Epoch 14/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 18.9826 - val_loss: 18.1001\n",
      "Epoch 15/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 16.8621 - val_loss: 16.1103\n",
      "Epoch 16/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 15.0603 - val_loss: 14.4648\n",
      "Epoch 17/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 13.5600 - val_loss: 13.0789\n",
      "Epoch 18/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 12.2868 - val_loss: 11.8913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 11.1870 - val_loss: 10.8560\n",
      "Epoch 20/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 10.2228 - val_loss: 9.9425\n",
      "Epoch 21/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 9.3676 - val_loss: 9.1288\n",
      "Epoch 22/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 8.6025 - val_loss: 8.3961\n",
      "Epoch 23/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 7.9127 - val_loss: 7.7347\n",
      "Epoch 24/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 7.2874 - val_loss: 7.1313\n",
      "Epoch 25/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 6.7174 - val_loss: 6.5816\n",
      "Epoch 26/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 6.1961 - val_loss: 6.0775\n",
      "Epoch 27/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 5.7178 - val_loss: 5.6133\n",
      "Epoch 28/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 5.2775 - val_loss: 5.1855\n",
      "Epoch 29/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.8715 - val_loss: 4.7909\n",
      "Epoch 30/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.4962 - val_loss: 4.4256\n",
      "Epoch 31/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.1487 - val_loss: 4.0870\n",
      "Epoch 32/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.8268 - val_loss: 3.7724\n",
      "Epoch 33/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.5282 - val_loss: 3.4801\n",
      "Epoch 34/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.2510 - val_loss: 3.2097\n",
      "Epoch 35/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.9935 - val_loss: 2.9570\n",
      "Epoch 36/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.7543 - val_loss: 2.7227\n",
      "Epoch 37/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.5319 - val_loss: 2.5046\n",
      "Epoch 38/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.3254 - val_loss: 2.3019\n",
      "Epoch 39/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.1335 - val_loss: 2.1134\n",
      "Epoch 40/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.9552 - val_loss: 1.9383\n",
      "Epoch 41/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.7897 - val_loss: 1.7759\n",
      "Epoch 42/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.6363 - val_loss: 1.6243\n",
      "Epoch 43/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.4941 - val_loss: 1.4842\n",
      "Epoch 44/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.3625 - val_loss: 1.3546\n",
      "Epoch 45/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.2407 - val_loss: 1.2351\n",
      "Epoch 46/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.1282 - val_loss: 1.1238\n",
      "Epoch 47/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.0244 - val_loss: 1.0215\n",
      "Epoch 48/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.9289 - val_loss: 0.9270\n",
      "Epoch 49/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.8410 - val_loss: 0.8402\n",
      "Epoch 50/300\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 0.7604 - val_loss: 0.7607\n",
      "Epoch 51/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6867 - val_loss: 0.6876\n",
      "Epoch 52/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6193 - val_loss: 0.6211\n",
      "Epoch 53/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5579 - val_loss: 0.5600\n",
      "Epoch 54/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5022 - val_loss: 0.5047\n",
      "Epoch 55/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4518 - val_loss: 0.4546\n",
      "Epoch 56/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4063 - val_loss: 0.4091\n",
      "Epoch 57/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.3654 - val_loss: 0.3686\n",
      "Epoch 58/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 0.3317\n",
      "Epoch 59/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2960 - val_loss: 0.2993\n",
      "Epoch 60/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2669 - val_loss: 0.2700\n",
      "Epoch 61/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2412 - val_loss: 0.2441\n",
      "Epoch 62/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2186 - val_loss: 0.2214\n",
      "Epoch 63/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 0.2011\n",
      "Epoch 64/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1816 - val_loss: 0.1839\n",
      "Epoch 65/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1668 - val_loss: 0.1687\n",
      "Epoch 66/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1541 - val_loss: 0.1556\n",
      "Epoch 67/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1431 - val_loss: 0.1444\n",
      "Epoch 68/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1339 - val_loss: 0.1346\n",
      "Epoch 69/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1265\n",
      "Epoch 70/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1197 - val_loss: 0.1197\n",
      "Epoch 71/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 0.1138\n",
      "Epoch 72/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1099 - val_loss: 0.1091\n",
      "Epoch 73/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1063 - val_loss: 0.1050\n",
      "Epoch 74/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1017\n",
      "Epoch 75/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 0.0989\n",
      "Epoch 76/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0990 - val_loss: 0.0967\n",
      "Epoch 77/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0975 - val_loss: 0.0948\n",
      "Epoch 78/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0962 - val_loss: 0.0932\n",
      "Epoch 79/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 0.0919\n",
      "Epoch 80/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0941 - val_loss: 0.0907\n",
      "Epoch 81/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0933 - val_loss: 0.0896\n",
      "Epoch 82/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0925 - val_loss: 0.0887\n",
      "Epoch 83/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.0879\n",
      "Epoch 84/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 0.0870\n",
      "Epoch 85/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.0863\n",
      "Epoch 86/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0898 - val_loss: 0.0856\n",
      "Epoch 87/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0891 - val_loss: 0.0849\n",
      "Epoch 88/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0884 - val_loss: 0.0841\n",
      "Epoch 89/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 0.0834\n",
      "Epoch 90/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 0.0826\n",
      "Epoch 91/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.0818\n",
      "Epoch 92/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 0.0811\n",
      "Epoch 93/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 0.0803\n",
      "Epoch 94/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 0.0795\n",
      "Epoch 95/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0828 - val_loss: 0.0787\n",
      "Epoch 96/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0820 - val_loss: 0.0779\n",
      "Epoch 97/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0811 - val_loss: 0.0769\n",
      "Epoch 98/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 0.0761\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0792 - val_loss: 0.0752\n",
      "Epoch 100/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.0742\n",
      "Epoch 101/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0733\n",
      "Epoch 102/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 0.0724\n",
      "Epoch 103/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 0.0714\n",
      "Epoch 104/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0741 - val_loss: 0.0703\n",
      "Epoch 105/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0694\n",
      "Epoch 106/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0683\n",
      "Epoch 107/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.0673\n",
      "Epoch 108/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.0662\n",
      "Epoch 109/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.0652\n",
      "Epoch 110/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.0642\n",
      "Epoch 111/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0632\n",
      "Epoch 112/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.0620\n",
      "Epoch 113/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.0610\n",
      "Epoch 114/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.0600\n",
      "Epoch 115/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0619 - val_loss: 0.0588\n",
      "Epoch 116/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0578\n",
      "Epoch 117/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.0567\n",
      "Epoch 118/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0585 - val_loss: 0.0558\n",
      "Epoch 119/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.0547\n",
      "Epoch 120/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0563 - val_loss: 0.0537\n",
      "Epoch 121/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.0526\n",
      "Epoch 122/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.0517\n",
      "Epoch 123/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.0507\n",
      "Epoch 124/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0497\n",
      "Epoch 125/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.0487\n",
      "Epoch 126/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.0478\n",
      "Epoch 127/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.0468\n",
      "Epoch 128/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.0460\n",
      "Epoch 129/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.0451\n",
      "Epoch 130/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.0443\n",
      "Epoch 131/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.0435\n",
      "Epoch 132/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.0428\n",
      "Epoch 133/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.0420\n",
      "Epoch 134/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0429 - val_loss: 0.0412\n",
      "Epoch 135/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.0407\n",
      "Epoch 136/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.0399\n",
      "Epoch 137/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0391\n",
      "Epoch 138/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0386\n",
      "Epoch 139/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.0380\n",
      "Epoch 140/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0374\n",
      "Epoch 141/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0368\n",
      "Epoch 142/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.0363\n",
      "Epoch 143/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0358\n",
      "Epoch 144/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0353\n",
      "Epoch 145/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0348\n",
      "Epoch 146/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0353 - val_loss: 0.0345\n",
      "Epoch 147/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0340\n",
      "Epoch 148/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0336\n",
      "Epoch 149/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0333\n",
      "Epoch 150/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0329\n",
      "Epoch 151/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0326\n",
      "Epoch 152/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0322\n",
      "Epoch 153/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0319\n",
      "Epoch 154/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0316\n",
      "Epoch 155/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 156/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0310\n",
      "Epoch 157/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0308\n",
      "Epoch 158/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0305\n",
      "Epoch 159/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.0303\n",
      "Epoch 160/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0301\n",
      "Epoch 161/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0298\n",
      "Epoch 162/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0296\n",
      "Epoch 163/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0294\n",
      "Epoch 164/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0292\n",
      "Epoch 165/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0291\n",
      "Epoch 166/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0288\n",
      "Epoch 167/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0287\n",
      "Epoch 168/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0286\n",
      "Epoch 169/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0283\n",
      "Epoch 170/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0282\n",
      "Epoch 171/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0281\n",
      "Epoch 172/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0279\n",
      "Epoch 173/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0278\n",
      "Epoch 174/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0276\n",
      "Epoch 175/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0274\n",
      "Epoch 176/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0273\n",
      "Epoch 177/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0272\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0271\n",
      "Epoch 179/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0269\n",
      "Epoch 180/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0268\n",
      "Epoch 181/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0268\n",
      "Epoch 182/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0266\n",
      "Epoch 183/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0265\n",
      "Epoch 184/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0264\n",
      "Epoch 185/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0263\n",
      "Epoch 186/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0262\n",
      "Epoch 187/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0261\n",
      "Epoch 188/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0260\n",
      "Epoch 189/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0259\n",
      "Epoch 190/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0230 - val_loss: 0.0258\n",
      "Epoch 191/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0257\n",
      "Epoch 192/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0257\n",
      "Epoch 193/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0256\n",
      "Epoch 194/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 195/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0254\n",
      "Epoch 196/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0254\n",
      "Epoch 197/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0253\n",
      "Epoch 198/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0252\n",
      "Epoch 199/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0252\n",
      "Epoch 200/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0251\n",
      "Epoch 201/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0250\n",
      "Epoch 202/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0249\n",
      "Epoch 203/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0210 - val_loss: 0.0249\n",
      "Epoch 204/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0248\n",
      "Epoch 205/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0208 - val_loss: 0.0248\n",
      "Epoch 206/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0207 - val_loss: 0.0247\n",
      "Epoch 207/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0205 - val_loss: 0.0246\n",
      "Epoch 208/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0204 - val_loss: 0.0246\n",
      "Epoch 209/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0246\n",
      "Epoch 210/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0202 - val_loss: 0.0245\n",
      "Epoch 211/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0245\n",
      "Epoch 212/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0244\n",
      "Epoch 213/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0244\n",
      "Epoch 214/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0244\n",
      "Epoch 215/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0196 - val_loss: 0.0243\n",
      "Epoch 216/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.0243\n",
      "Epoch 217/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0242\n",
      "Epoch 218/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0193 - val_loss: 0.0242\n",
      "Epoch 219/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.0241\n",
      "Epoch 220/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.0241\n",
      "Epoch 221/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0241\n",
      "Epoch 222/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0241\n",
      "Epoch 223/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0240\n",
      "Epoch 224/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0240\n",
      "Epoch 225/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0240\n",
      "Epoch 226/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0239\n",
      "Epoch 227/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0239\n",
      "Epoch 228/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0239\n",
      "Epoch 229/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0239\n",
      "Epoch 230/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0240\n",
      "Epoch 231/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0181 - val_loss: 0.0238\n",
      "Epoch 232/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0181 - val_loss: 0.0238\n",
      "Epoch 233/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.0238\n",
      "Epoch 234/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0237\n",
      "Epoch 235/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0240\n",
      "Epoch 236/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0237\n",
      "Epoch 237/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 0.0237\n",
      "Epoch 238/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0176 - val_loss: 0.0236\n",
      "Epoch 239/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0236\n",
      "Epoch 240/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0236\n",
      "Epoch 241/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0237\n",
      "Epoch 242/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0236\n",
      "Epoch 243/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0236\n",
      "Epoch 244/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0236\n",
      "Epoch 245/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.0235\n",
      "Epoch 246/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.0235\n",
      "Epoch 247/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.0235\n",
      "Epoch 248/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.0236\n",
      "Epoch 249/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 0.0235\n",
      "Epoch 250/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0169 - val_loss: 0.0235\n",
      "Epoch 251/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0236\n",
      "Epoch 252/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0235\n",
      "Epoch 253/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.0234\n",
      "Epoch 254/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.0234\n",
      "Epoch 255/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.0235\n",
      "Epoch 256/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0234\n",
      "Epoch 257/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0234\n",
      "Epoch 258/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.0234\n",
      "Epoch 259/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0234\n",
      "Epoch 260/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0234\n",
      "Epoch 261/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0233\n",
      "Epoch 262/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0234\n",
      "Epoch 263/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0234\n",
      "Epoch 264/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0234\n",
      "Epoch 265/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0233\n",
      "Epoch 266/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0233\n",
      "Epoch 267/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0233\n",
      "Epoch 268/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0233\n",
      "Epoch 269/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.0233\n",
      "Epoch 270/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.0233\n",
      "Epoch 271/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.0233\n",
      "Epoch 272/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0159 - val_loss: 0.0233\n",
      "Epoch 273/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.0232\n",
      "Epoch 274/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0233\n",
      "Epoch 275/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0232\n",
      "Epoch 276/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0232\n",
      "Epoch 277/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.0232\n",
      "Epoch 278/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.0232\n",
      "Epoch 279/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.0232\n",
      "Epoch 280/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0156 - val_loss: 0.0232\n",
      "Epoch 281/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.0232\n",
      "Epoch 282/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.0232\n",
      "Epoch 283/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.0232\n",
      "Epoch 284/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.0232\n",
      "Epoch 285/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.0232\n",
      "Epoch 286/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0155 - val_loss: 0.0231\n",
      "Epoch 287/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0154 - val_loss: 0.0231\n",
      "Epoch 288/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.0231\n",
      "Epoch 289/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.0231\n",
      "Epoch 290/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.0231\n",
      "Epoch 291/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0230\n",
      "Epoch 292/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0230\n",
      "Epoch 293/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.0230\n",
      "Epoch 294/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0230\n",
      "Epoch 295/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0229\n",
      "Epoch 296/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.0230\n",
      "Epoch 297/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.0230\n",
      "Epoch 298/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.0229\n",
      "Epoch 299/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.0229\n",
      "Epoch 300/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0150 - val_loss: 0.0229\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0229\n",
      "Epoch 1/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 92.3638 - val_loss: 69.2312\n",
      "Epoch 2/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 58.1253 - val_loss: 49.5999\n",
      "Epoch 3/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 44.0641 - val_loss: 39.4093\n",
      "Epoch 4/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 35.9255 - val_loss: 32.8636\n",
      "Epoch 5/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 30.3827 - val_loss: 28.1521\n",
      "Epoch 6/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 26.2571 - val_loss: 24.5317\n",
      "Epoch 7/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 23.0187 - val_loss: 21.6298\n",
      "Epoch 8/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 20.3858 - val_loss: 19.2383\n",
      "Epoch 9/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 18.1917 - val_loss: 17.2212\n",
      "Epoch 10/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 16.3278 - val_loss: 15.4950\n",
      "Epoch 11/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 14.7218 - val_loss: 13.9996\n",
      "Epoch 12/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 13.3219 - val_loss: 12.6875\n",
      "Epoch 13/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 12.0899 - val_loss: 11.5283\n",
      "Epoch 14/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 10.9973 - val_loss: 10.4964\n",
      "Epoch 15/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 10.0220 - val_loss: 9.5721\n",
      "Epoch 16/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 9.1466 - val_loss: 8.7412\n",
      "Epoch 17/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 8.3574 - val_loss: 7.9903\n",
      "Epoch 18/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 7.6426 - val_loss: 7.3097\n",
      "Epoch 19/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 6.9934 - val_loss: 6.6893\n",
      "Epoch 20/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 6.4017 - val_loss: 6.1239\n",
      "Epoch 21/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 5.8614 - val_loss: 5.6057\n",
      "Epoch 22/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 5.3667 - val_loss: 5.1326\n",
      "Epoch 23/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 4.9133 - val_loss: 4.6970\n",
      "Epoch 24/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 4.4968 - val_loss: 4.2971\n",
      "Epoch 25/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 4.1141 - val_loss: 3.9292\n",
      "Epoch 26/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 3.7618 - val_loss: 3.5907\n",
      "Epoch 27/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 3.4375 - val_loss: 3.2786\n",
      "Epoch 28/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 3.1387 - val_loss: 2.9918\n",
      "Epoch 29/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.8633 - val_loss: 2.7265\n",
      "Epoch 30/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.6095 - val_loss: 2.4824\n",
      "Epoch 31/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 2.3757 - val_loss: 2.2572\n",
      "Epoch 32/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 2.1603 - val_loss: 2.0501\n",
      "Epoch 33/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1.9619 - val_loss: 1.8599\n",
      "Epoch 34/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1.7795 - val_loss: 1.6840\n",
      "Epoch 35/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1.6116 - val_loss: 1.5231\n",
      "Epoch 36/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 4ms/step - loss: 1.4574 - val_loss: 1.3751\n",
      "Epoch 37/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1.3160 - val_loss: 1.2390\n",
      "Epoch 38/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1.1865 - val_loss: 1.1147\n",
      "Epoch 39/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.0681 - val_loss: 1.0012\n",
      "Epoch 40/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.9600 - val_loss: 0.8974\n",
      "Epoch 41/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8615 - val_loss: 0.8035\n",
      "Epoch 42/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.7721 - val_loss: 0.7179\n",
      "Epoch 43/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.6909 - val_loss: 0.6403\n",
      "Epoch 44/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.6175 - val_loss: 0.5698\n",
      "Epoch 45/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5512 - val_loss: 0.5070\n",
      "Epoch 46/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4919 - val_loss: 0.4504\n",
      "Epoch 47/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4386 - val_loss: 0.3999\n",
      "Epoch 48/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.3911 - val_loss: 0.3546\n",
      "Epoch 49/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.3488 - val_loss: 0.3148\n",
      "Epoch 50/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.3114 - val_loss: 0.2795\n",
      "Epoch 51/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.2785 - val_loss: 0.2484\n",
      "Epoch 52/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2496 - val_loss: 0.2212\n",
      "Epoch 53/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 0.1974\n",
      "Epoch 54/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2027 - val_loss: 0.1771\n",
      "Epoch 55/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1839 - val_loss: 0.1596\n",
      "Epoch 56/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 0.1446\n",
      "Epoch 57/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1542 - val_loss: 0.1319\n",
      "Epoch 58/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1427 - val_loss: 0.1212\n",
      "Epoch 59/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 0.1124\n",
      "Epoch 60/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1050\n",
      "Epoch 61/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 0.0990\n",
      "Epoch 62/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1132 - val_loss: 0.0941\n",
      "Epoch 63/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1088 - val_loss: 0.0900\n",
      "Epoch 64/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 0.0869\n",
      "Epoch 65/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 0.0843\n",
      "Epoch 66/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1000 - val_loss: 0.0822\n",
      "Epoch 67/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 0.0806\n",
      "Epoch 68/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0966 - val_loss: 0.0792\n",
      "Epoch 69/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0954 - val_loss: 0.0781\n",
      "Epoch 70/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0771\n",
      "Epoch 71/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 0.0764\n",
      "Epoch 72/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 0.0756\n",
      "Epoch 73/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.0750\n",
      "Epoch 74/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 0.0743\n",
      "Epoch 75/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 0.0737\n",
      "Epoch 76/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 0.0731\n",
      "Epoch 77/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0889 - val_loss: 0.0724\n",
      "Epoch 78/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 0.0718\n",
      "Epoch 79/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 0.0712\n",
      "Epoch 80/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0865 - val_loss: 0.0705\n",
      "Epoch 81/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0857 - val_loss: 0.0698\n",
      "Epoch 82/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0848 - val_loss: 0.0690\n",
      "Epoch 83/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0839 - val_loss: 0.0683\n",
      "Epoch 84/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0676\n",
      "Epoch 85/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0821 - val_loss: 0.0668\n",
      "Epoch 86/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.0660\n",
      "Epoch 87/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 0.0651\n",
      "Epoch 88/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0643\n",
      "Epoch 89/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0635\n",
      "Epoch 90/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0769 - val_loss: 0.0626\n",
      "Epoch 91/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0758 - val_loss: 0.0617\n",
      "Epoch 92/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0748 - val_loss: 0.0608\n",
      "Epoch 93/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0599\n",
      "Epoch 94/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0589\n",
      "Epoch 95/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0580\n",
      "Epoch 96/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.0571\n",
      "Epoch 97/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.0561\n",
      "Epoch 98/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.0551\n",
      "Epoch 99/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0663 - val_loss: 0.0542\n",
      "Epoch 100/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.0532\n",
      "Epoch 101/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0638 - val_loss: 0.0523\n",
      "Epoch 102/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.0513\n",
      "Epoch 103/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0504\n",
      "Epoch 104/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0494\n",
      "Epoch 105/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0484\n",
      "Epoch 106/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.0475\n",
      "Epoch 107/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.0466\n",
      "Epoch 108/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.0457\n",
      "Epoch 109/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.0448\n",
      "Epoch 110/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.0440\n",
      "Epoch 111/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0513 - val_loss: 0.0431\n",
      "Epoch 112/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0423\n",
      "Epoch 113/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.0415\n",
      "Epoch 114/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.0408\n",
      "Epoch 115/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.0400\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.0393\n",
      "Epoch 117/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0386\n",
      "Epoch 118/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0379\n",
      "Epoch 119/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.0373\n",
      "Epoch 120/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0367\n",
      "Epoch 121/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.0362\n",
      "Epoch 122/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.0356\n",
      "Epoch 123/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.0351\n",
      "Epoch 124/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0347\n",
      "Epoch 125/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0342\n",
      "Epoch 126/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0337\n",
      "Epoch 127/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0357 - val_loss: 0.0333\n",
      "Epoch 128/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0328\n",
      "Epoch 129/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0325\n",
      "Epoch 130/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0322\n",
      "Epoch 131/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0320\n",
      "Epoch 132/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0315\n",
      "Epoch 133/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0312\n",
      "Epoch 134/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0314 - val_loss: 0.0310\n",
      "Epoch 135/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "Epoch 136/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0303\n",
      "Epoch 137/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0301\n",
      "Epoch 138/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0299\n",
      "Epoch 139/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0298\n",
      "Epoch 140/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0295\n",
      "Epoch 141/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0293\n",
      "Epoch 142/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0290\n",
      "Epoch 143/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0289\n",
      "Epoch 144/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0286\n",
      "Epoch 145/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0283\n",
      "Epoch 146/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0283\n",
      "Epoch 147/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0281\n",
      "Epoch 148/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0278\n",
      "Epoch 149/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0278\n",
      "Epoch 150/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0275\n",
      "Epoch 151/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0274\n",
      "Epoch 152/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0273\n",
      "Epoch 153/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0271\n",
      "Epoch 154/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0270\n",
      "Epoch 155/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0267\n",
      "Epoch 156/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0272\n",
      "Epoch 157/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0265\n",
      "Epoch 158/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0266\n",
      "Epoch 159/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0263\n",
      "Epoch 160/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0265\n",
      "Epoch 161/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0261\n",
      "Epoch 162/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0260\n",
      "Epoch 163/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0259\n",
      "Epoch 164/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0259\n",
      "Epoch 165/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0213 - val_loss: 0.0256\n",
      "Epoch 166/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0255\n",
      "Epoch 167/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0256\n",
      "Epoch 168/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0254\n",
      "Epoch 169/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0253\n",
      "Epoch 170/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0254\n",
      "Epoch 171/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0252\n",
      "Epoch 172/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0251\n",
      "Epoch 173/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0250\n",
      "Epoch 174/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0251\n",
      "Epoch 175/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.0250\n",
      "Epoch 176/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.0250\n",
      "Epoch 177/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0249\n",
      "Epoch 178/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0248\n",
      "Epoch 179/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0185 - val_loss: 0.0248\n",
      "Epoch 180/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0248\n",
      "Epoch 181/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.0247\n",
      "Epoch 182/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.0247\n",
      "Epoch 183/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0249\n",
      "Epoch 184/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0177 - val_loss: 0.0247\n",
      "Epoch 185/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0175 - val_loss: 0.0248\n",
      "Epoch 186/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0246\n",
      "Epoch 187/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.0246\n",
      "Epoch 188/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.0246\n",
      "Epoch 189/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 0.0247\n",
      "Epoch 190/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0247\n",
      "Epoch 191/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.0246\n",
      "Epoch 192/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0247\n",
      "Epoch 193/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0246\n",
      "Epoch 194/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0246\n",
      "Epoch 195/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0247\n",
      "Epoch 196/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0247\n",
      "Epoch 197/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.0247\n",
      "Epoch 198/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0247\n",
      "Epoch 199/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0157 - val_loss: 0.0247\n",
      "Epoch 200/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0156 - val_loss: 0.0247\n",
      "Epoch 201/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.0250\n",
      "Epoch 202/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0155 - val_loss: 0.0247\n",
      "Epoch 203/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.0248\n",
      "Epoch 204/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.0248\n",
      "Epoch 205/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0247\n",
      "Epoch 206/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.0248\n",
      "Epoch 207/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.0248\n",
      "Epoch 208/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.0249\n",
      "Epoch 209/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.0249\n",
      "Epoch 210/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0148 - val_loss: 0.0248\n",
      "Epoch 211/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0249\n",
      "Epoch 212/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0146 - val_loss: 0.0249\n",
      "Epoch 213/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0145 - val_loss: 0.0249\n",
      "Epoch 214/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.0249\n",
      "Epoch 215/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.0250\n",
      "Epoch 216/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0143 - val_loss: 0.0250\n",
      "Epoch 217/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 0.0252\n",
      "Epoch 218/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0141 - val_loss: 0.0250\n",
      "Epoch 219/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.0250\n",
      "Epoch 220/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0140 - val_loss: 0.0250\n",
      "Epoch 221/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0140 - val_loss: 0.0250\n",
      "Epoch 222/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0139 - val_loss: 0.0250\n",
      "Epoch 223/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.0250\n",
      "Epoch 224/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.0251\n",
      "Epoch 225/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.0252\n",
      "Epoch 226/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0137 - val_loss: 0.0252\n",
      "Epoch 227/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.0251\n",
      "Epoch 228/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0253\n",
      "Epoch 229/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0252\n",
      "Epoch 230/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0252\n",
      "Epoch 231/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0252\n",
      "Epoch 232/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.0252\n",
      "Epoch 233/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0252\n",
      "Epoch 234/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0254\n",
      "Epoch 235/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0254\n",
      "Epoch 236/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0254\n",
      "Epoch 237/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.0254\n",
      "Epoch 238/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0254\n",
      "Epoch 239/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0255\n",
      "Epoch 240/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0254\n",
      "Epoch 241/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0254\n",
      "Epoch 242/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0255\n",
      "Epoch 243/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0256\n",
      "Epoch 244/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0255\n",
      "Epoch 245/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0255\n",
      "Epoch 246/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0255\n",
      "Epoch 247/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.0256\n",
      "Epoch 248/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0126 - val_loss: 0.0256\n",
      "Epoch 249/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0126 - val_loss: 0.0256\n",
      "Epoch 250/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0256\n",
      "Epoch 251/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0257\n",
      "Epoch 252/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0257\n",
      "Epoch 253/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0257\n",
      "Epoch 254/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0257\n",
      "Epoch 255/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0258\n",
      "Epoch 256/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0258\n",
      "Epoch 257/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0258\n",
      "Epoch 258/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0258\n",
      "Epoch 259/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0258\n",
      "Epoch 260/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0259\n",
      "Epoch 261/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0259\n",
      "Epoch 262/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0259\n",
      "Epoch 263/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0259\n",
      "Epoch 264/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0259\n",
      "Epoch 265/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0259\n",
      "Epoch 266/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0260\n",
      "Epoch 267/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0260\n",
      "Epoch 268/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0261\n",
      "Epoch 269/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0260\n",
      "Epoch 270/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0260\n",
      "Epoch 271/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0261\n",
      "Epoch 272/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0261\n",
      "Epoch 273/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0261\n",
      "Epoch 274/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0262\n",
      "Epoch 275/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0263\n",
      "Epoch 276/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0262\n",
      "Epoch 277/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0262\n",
      "Epoch 278/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0263\n",
      "Epoch 279/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0264\n",
      "Epoch 280/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0263\n",
      "Epoch 281/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0264\n",
      "Epoch 282/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0264\n",
      "Epoch 283/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0114 - val_loss: 0.0264\n",
      "Epoch 284/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0264\n",
      "Epoch 285/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0265\n",
      "Epoch 286/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0265\n",
      "Epoch 287/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0266\n",
      "Epoch 288/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0266\n",
      "Epoch 289/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0266\n",
      "Epoch 290/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.0268\n",
      "Epoch 291/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0266\n",
      "Epoch 292/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0266\n",
      "Epoch 293/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0269\n",
      "Epoch 294/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0268\n",
      "Epoch 295/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0111 - val_loss: 0.0267\n",
      "Epoch 296/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0268\n",
      "Epoch 297/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0268\n",
      "Epoch 298/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0268\n",
      "Epoch 299/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0268\n",
      "Epoch 300/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.0269\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0269\n",
      "Epoch 1/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 120.0449 - val_loss: 97.3956\n",
      "Epoch 2/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 83.1934 - val_loss: 71.5379\n",
      "Epoch 3/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 62.2736 - val_loss: 53.7172\n",
      "Epoch 4/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 48.1182 - val_loss: 43.0914\n",
      "Epoch 5/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 39.5834 - val_loss: 36.2254\n",
      "Epoch 6/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 33.7382 - val_loss: 31.2550\n",
      "Epoch 7/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 29.3650 - val_loss: 27.4192\n",
      "Epoch 8/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 25.9162 - val_loss: 24.3308\n",
      "Epoch 9/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 23.0987 - val_loss: 21.7728\n",
      "Epoch 10/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 20.7397 - val_loss: 19.6051\n",
      "Epoch 11/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 18.7268 - val_loss: 17.7462\n",
      "Epoch 12/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 16.9848 - val_loss: 16.1230\n",
      "Epoch 13/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 15.4592 - val_loss: 14.6967\n",
      "Epoch 14/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 14.1109 - val_loss: 13.4305\n",
      "Epoch 15/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 12.9099 - val_loss: 12.2985\n",
      "Epoch 16/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 11.8332 - val_loss: 11.2805\n",
      "Epoch 17/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 10.8623 - val_loss: 10.3605\n",
      "Epoch 18/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 9.9829 - val_loss: 9.5254\n",
      "Epoch 19/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 9.1831 - val_loss: 8.7644\n",
      "Epoch 20/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 8.4531 - val_loss: 8.0689\n",
      "Epoch 21/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 7.7847 - val_loss: 7.4324\n",
      "Epoch 22/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 7.1715 - val_loss: 6.8462\n",
      "Epoch 23/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 6.6072 - val_loss: 6.3071\n",
      "Epoch 24/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 6.0874 - val_loss: 5.8103\n",
      "Epoch 25/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 5.6076 - val_loss: 5.3511\n",
      "Epoch 26/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 5.1641 - val_loss: 4.9268\n",
      "Epoch 27/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 4.7538 - val_loss: 4.5337\n",
      "Epoch 28/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 4.3737 - val_loss: 4.1700\n",
      "Epoch 29/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 4.0215 - val_loss: 3.8327\n",
      "Epoch 30/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 3.6950 - val_loss: 3.5201\n",
      "Epoch 31/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 3.3923 - val_loss: 3.2298\n",
      "Epoch 32/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 3.1114 - val_loss: 2.9614\n",
      "Epoch 33/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.8509 - val_loss: 2.7121\n",
      "Epoch 34/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.6093 - val_loss: 2.4809\n",
      "Epoch 35/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 2.3854 - val_loss: 2.2663\n",
      "Epoch 36/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 2.1779 - val_loss: 2.0688\n",
      "Epoch 37/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1.9858 - val_loss: 1.8851\n",
      "Epoch 38/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.8081 - val_loss: 1.7153\n",
      "Epoch 39/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.6438 - val_loss: 1.5590\n",
      "Epoch 40/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.4922 - val_loss: 1.4148\n",
      "Epoch 41/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.3524 - val_loss: 1.2815\n",
      "Epoch 42/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.2236 - val_loss: 1.1592\n",
      "Epoch 43/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 1.1053 - val_loss: 1.0467\n",
      "Epoch 44/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.9967 - val_loss: 0.9443\n",
      "Epoch 45/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.8974 - val_loss: 0.8501\n",
      "Epoch 46/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8065 - val_loss: 0.7644\n",
      "Epoch 47/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.7237 - val_loss: 0.6867\n",
      "Epoch 48/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6485 - val_loss: 0.6157\n",
      "Epoch 49/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5803 - val_loss: 0.5518\n",
      "Epoch 50/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5187 - val_loss: 0.4943\n",
      "Epoch 51/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4632 - val_loss: 0.4426\n",
      "Epoch 52/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.4133 - val_loss: 0.3961\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 4ms/step - loss: 0.3687 - val_loss: 0.3548\n",
      "Epoch 54/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.3291 - val_loss: 0.3179\n",
      "Epoch 55/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2940 - val_loss: 0.2857\n",
      "Epoch 56/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2629 - val_loss: 0.2575\n",
      "Epoch 57/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2357 - val_loss: 0.2327\n",
      "Epoch 58/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 0.2110\n",
      "Epoch 59/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 0.1923\n",
      "Epoch 60/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1734 - val_loss: 0.1762\n",
      "Epoch 61/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1582 - val_loss: 0.1628\n",
      "Epoch 62/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1452 - val_loss: 0.1513\n",
      "Epoch 63/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1343 - val_loss: 0.1417\n",
      "Epoch 64/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1337\n",
      "Epoch 65/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1175 - val_loss: 0.1271\n",
      "Epoch 66/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1112 - val_loss: 0.1217\n",
      "Epoch 67/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1060 - val_loss: 0.1173\n",
      "Epoch 68/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1019 - val_loss: 0.1138\n",
      "Epoch 69/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 0.1110\n",
      "Epoch 70/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0958 - val_loss: 0.1086\n",
      "Epoch 71/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 0.1069\n",
      "Epoch 72/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.1054\n",
      "Epoch 73/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.1042\n",
      "Epoch 74/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0892 - val_loss: 0.1033\n",
      "Epoch 75/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 0.1024\n",
      "Epoch 76/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0874 - val_loss: 0.1016\n",
      "Epoch 77/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.1009\n",
      "Epoch 78/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0860 - val_loss: 0.1002\n",
      "Epoch 79/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0853 - val_loss: 0.0995\n",
      "Epoch 80/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.0988\n",
      "Epoch 81/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 0.0981\n",
      "Epoch 82/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0833 - val_loss: 0.0973\n",
      "Epoch 83/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 0.0966\n",
      "Epoch 84/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 0.0958\n",
      "Epoch 85/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 0.0950\n",
      "Epoch 86/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0805 - val_loss: 0.0941\n",
      "Epoch 87/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0797 - val_loss: 0.0932\n",
      "Epoch 88/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 0.0923\n",
      "Epoch 89/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.0914\n",
      "Epoch 90/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.0904\n",
      "Epoch 91/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0764 - val_loss: 0.0894\n",
      "Epoch 92/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 0.0884\n",
      "Epoch 93/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0747 - val_loss: 0.0873\n",
      "Epoch 94/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0737 - val_loss: 0.0863\n",
      "Epoch 95/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0852\n",
      "Epoch 96/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0841\n",
      "Epoch 97/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.0830\n",
      "Epoch 98/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.0818\n",
      "Epoch 99/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.0806\n",
      "Epoch 100/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.0795\n",
      "Epoch 101/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.0783\n",
      "Epoch 102/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0657 - val_loss: 0.0770\n",
      "Epoch 103/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.0758\n",
      "Epoch 104/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.0746\n",
      "Epoch 105/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.0734\n",
      "Epoch 106/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0722\n",
      "Epoch 107/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0709\n",
      "Epoch 108/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.0697\n",
      "Epoch 109/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0684\n",
      "Epoch 110/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.0672\n",
      "Epoch 111/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.0660\n",
      "Epoch 112/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.0648\n",
      "Epoch 113/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0541 - val_loss: 0.0636\n",
      "Epoch 114/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.0624\n",
      "Epoch 115/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0612\n",
      "Epoch 116/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.0600\n",
      "Epoch 117/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.0588\n",
      "Epoch 118/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.0576\n",
      "Epoch 119/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.0564\n",
      "Epoch 120/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.0551\n",
      "Epoch 121/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.0539\n",
      "Epoch 122/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0526\n",
      "Epoch 123/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0514\n",
      "Epoch 124/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.0502\n",
      "Epoch 125/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.0490\n",
      "Epoch 126/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0407 - val_loss: 0.0478\n",
      "Epoch 127/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0467\n",
      "Epoch 128/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.0457\n",
      "Epoch 129/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.0447\n",
      "Epoch 130/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.0437\n",
      "Epoch 131/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0428\n",
      "Epoch 132/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0420\n",
      "Epoch 133/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0413\n",
      "Epoch 134/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0405\n",
      "Epoch 135/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0397\n",
      "Epoch 136/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0390\n",
      "Epoch 137/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0384\n",
      "Epoch 138/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0378\n",
      "Epoch 139/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0372\n",
      "Epoch 140/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0366\n",
      "Epoch 141/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0361\n",
      "Epoch 142/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0355\n",
      "Epoch 143/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0350\n",
      "Epoch 144/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0345\n",
      "Epoch 145/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0340\n",
      "Epoch 146/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0336\n",
      "Epoch 147/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.0331\n",
      "Epoch 148/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0327\n",
      "Epoch 149/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0323\n",
      "Epoch 150/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0319\n",
      "Epoch 151/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0318\n",
      "Epoch 152/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0313\n",
      "Epoch 153/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0309\n",
      "Epoch 154/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0305\n",
      "Epoch 155/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0304\n",
      "Epoch 156/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0299\n",
      "Epoch 157/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0296\n",
      "Epoch 158/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0295\n",
      "Epoch 159/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0292\n",
      "Epoch 160/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0289\n",
      "Epoch 161/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0286\n",
      "Epoch 162/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0284\n",
      "Epoch 163/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0283\n",
      "Epoch 164/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0281\n",
      "Epoch 165/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0278\n",
      "Epoch 166/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0276\n",
      "Epoch 167/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0276\n",
      "Epoch 168/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0274\n",
      "Epoch 169/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0271\n",
      "Epoch 170/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0270\n",
      "Epoch 171/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0267\n",
      "Epoch 172/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0267\n",
      "Epoch 173/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0265\n",
      "Epoch 174/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0263\n",
      "Epoch 175/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0262\n",
      "Epoch 176/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0260\n",
      "Epoch 177/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0259\n",
      "Epoch 178/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0257\n",
      "Epoch 179/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0256\n",
      "Epoch 180/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0223 - val_loss: 0.0255\n",
      "Epoch 181/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0222 - val_loss: 0.0254\n",
      "Epoch 182/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0253\n",
      "Epoch 183/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0220 - val_loss: 0.0252\n",
      "Epoch 184/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0251\n",
      "Epoch 185/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0249\n",
      "Epoch 186/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0249\n",
      "Epoch 187/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0247\n",
      "Epoch 188/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0247\n",
      "Epoch 189/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0246\n",
      "Epoch 190/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.0245\n",
      "Epoch 191/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0246\n",
      "Epoch 192/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0244\n",
      "Epoch 193/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0242\n",
      "Epoch 194/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0242\n",
      "Epoch 195/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0241\n",
      "Epoch 196/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0240\n",
      "Epoch 197/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0239\n",
      "Epoch 198/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0239\n",
      "Epoch 199/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0238\n",
      "Epoch 200/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0237\n",
      "Epoch 201/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0237\n",
      "Epoch 202/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0236\n",
      "Epoch 203/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0235\n",
      "Epoch 204/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0234\n",
      "Epoch 205/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0199 - val_loss: 0.0234\n",
      "Epoch 206/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 0.0234\n",
      "Epoch 207/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0199 - val_loss: 0.0233\n",
      "Epoch 208/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0197 - val_loss: 0.0233\n",
      "Epoch 209/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0232\n",
      "Epoch 210/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0231\n",
      "Epoch 211/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0231\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0230\n",
      "Epoch 213/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0230\n",
      "Epoch 214/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0229\n",
      "Epoch 215/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0229\n",
      "Epoch 216/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0229\n",
      "Epoch 217/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.0228\n",
      "Epoch 218/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0228\n",
      "Epoch 219/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0227\n",
      "Epoch 220/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0226\n",
      "Epoch 221/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0226\n",
      "Epoch 222/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0227\n",
      "Epoch 223/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0225\n",
      "Epoch 224/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0226\n",
      "Epoch 225/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0224\n",
      "Epoch 226/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0185 - val_loss: 0.0224\n",
      "Epoch 227/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0224\n",
      "Epoch 228/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0224\n",
      "Epoch 229/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0223\n",
      "Epoch 230/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0223\n",
      "Epoch 231/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0182 - val_loss: 0.0222\n",
      "Epoch 232/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0182 - val_loss: 0.0222\n",
      "Epoch 233/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0182 - val_loss: 0.0222\n",
      "Epoch 234/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0181 - val_loss: 0.0222\n",
      "Epoch 235/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.0221\n",
      "Epoch 236/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.0221\n",
      "Epoch 237/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0220\n",
      "Epoch 238/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0220\n",
      "Epoch 239/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0221\n",
      "Epoch 240/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0220\n",
      "Epoch 241/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 0.0219\n",
      "Epoch 242/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 0.0219\n",
      "Epoch 243/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 0.0218\n",
      "Epoch 244/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0176 - val_loss: 0.0219\n",
      "Epoch 245/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0218\n",
      "Epoch 246/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0218\n",
      "Epoch 247/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0218\n",
      "Epoch 248/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0217\n",
      "Epoch 249/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.0217\n",
      "Epoch 250/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.0217\n",
      "Epoch 251/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.0217\n",
      "Epoch 252/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.0216\n",
      "Epoch 253/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.0216\n",
      "Epoch 254/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0216\n",
      "Epoch 255/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.0215\n",
      "Epoch 256/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.0215\n",
      "Epoch 257/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.0215\n",
      "Epoch 258/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.0215\n",
      "Epoch 259/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.0215\n",
      "Epoch 260/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0170 - val_loss: 0.0214\n",
      "Epoch 261/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0214\n",
      "Epoch 262/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0215\n",
      "Epoch 263/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0214\n",
      "Epoch 264/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0169 - val_loss: 0.0213\n",
      "Epoch 265/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0169 - val_loss: 0.0214\n",
      "Epoch 266/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0169 - val_loss: 0.0213\n",
      "Epoch 267/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.0213\n",
      "Epoch 268/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0168 - val_loss: 0.0213\n",
      "Epoch 269/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.0214\n",
      "Epoch 270/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.0212\n",
      "Epoch 271/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.0212\n",
      "Epoch 272/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0212\n",
      "Epoch 273/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.0212\n",
      "Epoch 274/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0166 - val_loss: 0.0212\n",
      "Epoch 275/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0166 - val_loss: 0.0211\n",
      "Epoch 276/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0211\n",
      "Epoch 277/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0211\n",
      "Epoch 278/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0211\n",
      "Epoch 279/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.0212\n",
      "Epoch 280/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.0211\n",
      "Epoch 281/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0210\n",
      "Epoch 282/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0211\n",
      "Epoch 283/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0210\n",
      "Epoch 284/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0210\n",
      "Epoch 285/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0163 - val_loss: 0.0210\n",
      "Epoch 286/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0163 - val_loss: 0.0209\n",
      "Epoch 287/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0209\n",
      "Epoch 288/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0209\n",
      "Epoch 289/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0209\n",
      "Epoch 290/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 292/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 293/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 294/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 295/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0207\n",
      "Epoch 296/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0207\n",
      "Epoch 297/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0207\n",
      "Epoch 298/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0207\n",
      "Epoch 299/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0207\n",
      "Epoch 300/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0207\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 1/300\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 110.0188 - val_loss: 82.6875\n",
      "Epoch 2/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 67.6123 - val_loss: 56.2534\n",
      "Epoch 3/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 49.3001 - val_loss: 43.4780\n",
      "Epoch 4/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 39.1521 - val_loss: 35.3407\n",
      "Epoch 5/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 32.2691 - val_loss: 29.5234\n",
      "Epoch 6/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 27.1777 - val_loss: 24.9958\n",
      "Epoch 7/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 22.9449 - val_loss: 21.0343\n",
      "Epoch 8/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 19.3372 - val_loss: 17.7971\n",
      "Epoch 9/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 16.4357 - val_loss: 15.1894\n",
      "Epoch 10/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 14.0574 - val_loss: 13.0198\n",
      "Epoch 11/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 12.0718 - val_loss: 11.2083\n",
      "Epoch 12/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 10.4212 - val_loss: 9.7073\n",
      "Epoch 13/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 9.0518 - val_loss: 8.4561\n",
      "Epoch 14/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 7.9077 - val_loss: 7.4077\n",
      "Epoch 15/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 6.9428 - val_loss: 6.5187\n",
      "Epoch 16/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 6.1201 - val_loss: 5.7568\n",
      "Epoch 17/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 5.4121 - val_loss: 5.0973\n",
      "Epoch 18/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 4.7978 - val_loss: 4.5222\n",
      "Epoch 19/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 4.2613 - val_loss: 4.0191\n",
      "Epoch 20/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 3.7901 - val_loss: 3.5755\n",
      "Epoch 21/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 3.3739 - val_loss: 3.1835\n",
      "Epoch 22/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 3.0051 - val_loss: 2.8356\n",
      "Epoch 23/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 2.6774 - val_loss: 2.5249\n",
      "Epoch 24/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 2.3853 - val_loss: 2.2482\n",
      "Epoch 25/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 2.1245 - val_loss: 2.0011\n",
      "Epoch 26/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1.8915 - val_loss: 1.7802\n",
      "Epoch 27/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1.6829 - val_loss: 1.5816\n",
      "Epoch 28/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1.4961 - val_loss: 1.4047\n",
      "Epoch 29/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 1.3289 - val_loss: 1.2460\n",
      "Epoch 30/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 1.1792 - val_loss: 1.1039\n",
      "Epoch 31/300\n",
      "117/117 [==============================] - 1s 6ms/step - loss: 1.0454 - val_loss: 0.9766\n",
      "Epoch 32/300\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.9258 - val_loss: 0.8630\n",
      "Epoch 33/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8192 - val_loss: 0.7614\n",
      "Epoch 34/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.7241 - val_loss: 0.6716\n",
      "Epoch 35/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.6397 - val_loss: 0.5913\n",
      "Epoch 36/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.5649 - val_loss: 0.5205\n",
      "Epoch 37/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.4987 - val_loss: 0.4577\n",
      "Epoch 38/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.4404 - val_loss: 0.4027\n",
      "Epoch 39/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.3892 - val_loss: 0.3542\n",
      "Epoch 40/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.3444 - val_loss: 0.3120\n",
      "Epoch 41/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 0.2754\n",
      "Epoch 42/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2715 - val_loss: 0.2435\n",
      "Epoch 43/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.2424 - val_loss: 0.2157\n",
      "Epoch 44/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.2174 - val_loss: 0.1926\n",
      "Epoch 45/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1961 - val_loss: 0.1726\n",
      "Epoch 46/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1780 - val_loss: 0.1557\n",
      "Epoch 47/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.1627 - val_loss: 0.1416\n",
      "Epoch 48/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1500 - val_loss: 0.1300\n",
      "Epoch 49/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1395 - val_loss: 0.1199\n",
      "Epoch 50/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1308 - val_loss: 0.1119\n",
      "Epoch 51/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1054\n",
      "Epoch 52/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1180 - val_loss: 0.1002\n",
      "Epoch 53/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1134 - val_loss: 0.0958\n",
      "Epoch 54/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1097 - val_loss: 0.0925\n",
      "Epoch 55/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1068 - val_loss: 0.0898\n",
      "Epoch 56/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1045 - val_loss: 0.0878\n",
      "Epoch 57/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1026 - val_loss: 0.0859\n",
      "Epoch 58/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.1011 - val_loss: 0.0847\n",
      "Epoch 59/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0999 - val_loss: 0.0836\n",
      "Epoch 60/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.0826\n",
      "Epoch 61/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0980 - val_loss: 0.0818\n",
      "Epoch 62/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.0812\n",
      "Epoch 63/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0805\n",
      "Epoch 64/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.0799\n",
      "Epoch 65/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.0793\n",
      "Epoch 66/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0943 - val_loss: 0.0787\n",
      "Epoch 67/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 0.0781\n",
      "Epoch 68/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 0.0775\n",
      "Epoch 69/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0922 - val_loss: 0.0769\n",
      "Epoch 70/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 0.0763\n",
      "Epoch 71/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0906 - val_loss: 0.0757\n",
      "Epoch 72/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0898 - val_loss: 0.0751\n",
      "Epoch 73/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 0.0744\n",
      "Epoch 74/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0882 - val_loss: 0.0737\n",
      "Epoch 75/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 0.0731\n",
      "Epoch 76/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 0.0724\n",
      "Epoch 77/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 0.0717\n",
      "Epoch 78/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.0709\n",
      "Epoch 79/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 0.0702\n",
      "Epoch 80/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 0.0695\n",
      "Epoch 81/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 0.0687\n",
      "Epoch 82/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0808 - val_loss: 0.0679\n",
      "Epoch 83/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0797 - val_loss: 0.0671\n",
      "Epoch 84/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 0.0663\n",
      "Epoch 85/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.0654\n",
      "Epoch 86/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0643\n",
      "Epoch 87/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.0632\n",
      "Epoch 88/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0620\n",
      "Epoch 89/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0607\n",
      "Epoch 90/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0701 - val_loss: 0.0596\n",
      "Epoch 91/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.0582\n",
      "Epoch 92/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0669 - val_loss: 0.0570\n",
      "Epoch 93/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0652 - val_loss: 0.0558\n",
      "Epoch 94/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0636 - val_loss: 0.0546\n",
      "Epoch 95/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.0533\n",
      "Epoch 96/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0521\n",
      "Epoch 97/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.0509\n",
      "Epoch 98/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.0498\n",
      "Epoch 99/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.0488\n",
      "Epoch 100/300\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0477\n",
      "Epoch 101/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.0467\n",
      "Epoch 102/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.0458\n",
      "Epoch 103/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.0448\n",
      "Epoch 104/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
      "Epoch 105/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.0430\n",
      "Epoch 106/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.0423\n",
      "Epoch 107/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.0415\n",
      "Epoch 108/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.0408\n",
      "Epoch 109/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0400\n",
      "Epoch 110/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0429 - val_loss: 0.0393\n",
      "Epoch 111/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.0387\n",
      "Epoch 112/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.0382\n",
      "Epoch 113/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.0375\n",
      "Epoch 114/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.0370\n",
      "Epoch 115/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0364\n",
      "Epoch 116/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0359\n",
      "Epoch 117/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.0354\n",
      "Epoch 118/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.0349\n",
      "Epoch 119/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0345\n",
      "Epoch 120/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0340\n",
      "Epoch 121/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0336\n",
      "Epoch 122/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0331\n",
      "Epoch 123/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0327\n",
      "Epoch 124/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0325\n",
      "Epoch 125/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0319\n",
      "Epoch 126/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0316\n",
      "Epoch 127/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.0312\n",
      "Epoch 128/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0308\n",
      "Epoch 129/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.0305\n",
      "Epoch 130/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0315 - val_loss: 0.0302\n",
      "Epoch 131/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0298\n",
      "Epoch 132/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0296\n",
      "Epoch 133/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0293\n",
      "Epoch 134/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0290\n",
      "Epoch 135/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0286\n",
      "Epoch 136/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0283\n",
      "Epoch 137/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0282\n",
      "Epoch 138/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0279\n",
      "Epoch 139/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.0276\n",
      "Epoch 140/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0275\n",
      "Epoch 141/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0272\n",
      "Epoch 142/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0269\n",
      "Epoch 143/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0268\n",
      "Epoch 144/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0265\n",
      "Epoch 145/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0263\n",
      "Epoch 146/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0261\n",
      "Epoch 147/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.0259\n",
      "Epoch 148/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0257\n",
      "Epoch 149/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0253\n",
      "Epoch 151/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0252\n",
      "Epoch 152/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0250\n",
      "Epoch 153/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0249\n",
      "Epoch 154/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0247\n",
      "Epoch 155/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0245\n",
      "Epoch 156/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0244\n",
      "Epoch 157/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0243\n",
      "Epoch 158/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0241\n",
      "Epoch 159/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0241\n",
      "Epoch 160/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0239\n",
      "Epoch 161/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0237\n",
      "Epoch 162/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0236\n",
      "Epoch 163/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 0.0234\n",
      "Epoch 164/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0233\n",
      "Epoch 165/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0233\n",
      "Epoch 166/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0234\n",
      "Epoch 167/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0230\n",
      "Epoch 168/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0229\n",
      "Epoch 169/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0227\n",
      "Epoch 170/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0227\n",
      "Epoch 171/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0226\n",
      "Epoch 172/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0225\n",
      "Epoch 173/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0223\n",
      "Epoch 174/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.0223\n",
      "Epoch 175/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0224 - val_loss: 0.0222\n",
      "Epoch 176/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0223 - val_loss: 0.0221\n",
      "Epoch 177/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0220\n",
      "Epoch 178/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0221 - val_loss: 0.0218\n",
      "Epoch 179/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0217\n",
      "Epoch 180/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0216\n",
      "Epoch 181/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0217 - val_loss: 0.0215\n",
      "Epoch 182/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0214\n",
      "Epoch 183/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0213\n",
      "Epoch 184/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0213 - val_loss: 0.0212\n",
      "Epoch 185/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.0213\n",
      "Epoch 186/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0212\n",
      "Epoch 187/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0210 - val_loss: 0.0210\n",
      "Epoch 188/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\n",
      "Epoch 189/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\n",
      "Epoch 190/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0207 - val_loss: 0.0207\n",
      "Epoch 191/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0210\n",
      "Epoch 192/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0206\n",
      "Epoch 193/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0205\n",
      "Epoch 194/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0205\n",
      "Epoch 195/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0203\n",
      "Epoch 196/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0203\n",
      "Epoch 197/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 198/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0200 - val_loss: 0.0202\n",
      "Epoch 199/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0199 - val_loss: 0.0203\n",
      "Epoch 200/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0201\n",
      "Epoch 201/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 0.0201\n",
      "Epoch 202/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0199\n",
      "Epoch 203/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0198\n",
      "Epoch 204/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0197\n",
      "Epoch 205/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0198\n",
      "Epoch 206/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0200\n",
      "Epoch 207/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0195\n",
      "Epoch 208/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0195\n",
      "Epoch 209/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 210/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 211/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0193\n",
      "Epoch 212/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0193\n",
      "Epoch 213/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0193\n",
      "Epoch 214/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0191\n",
      "Epoch 215/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0192\n",
      "Epoch 216/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0190\n",
      "Epoch 217/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0189\n",
      "Epoch 218/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0189\n",
      "Epoch 219/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0189\n",
      "Epoch 220/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 221/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 222/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0189\n",
      "Epoch 223/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0182 - val_loss: 0.0192\n",
      "Epoch 224/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0182 - val_loss: 0.0187\n",
      "Epoch 225/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0182 - val_loss: 0.0187\n",
      "Epoch 226/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.0188\n",
      "Epoch 227/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.0186\n",
      "Epoch 228/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.0188\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0185\n",
      "Epoch 230/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0185\n",
      "Epoch 231/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.0186\n",
      "Epoch 232/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0178 - val_loss: 0.0183\n",
      "Epoch 233/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0178 - val_loss: 0.0183\n",
      "Epoch 234/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0177 - val_loss: 0.0183\n",
      "Epoch 235/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0182\n",
      "Epoch 236/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0176 - val_loss: 0.0182\n",
      "Epoch 237/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0182\n",
      "Epoch 238/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0183\n",
      "Epoch 239/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0181\n",
      "Epoch 240/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0181\n",
      "Epoch 241/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0189\n",
      "Epoch 242/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0183\n",
      "Epoch 243/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0180\n",
      "Epoch 244/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0179\n",
      "Epoch 245/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0172 - val_loss: 0.0182\n",
      "Epoch 246/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.0179\n",
      "Epoch 247/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.0178\n",
      "Epoch 248/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.0180\n",
      "Epoch 249/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.0178\n",
      "Epoch 250/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 251/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 252/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.0177\n",
      "Epoch 253/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 0.0179\n",
      "Epoch 254/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 0.0181\n",
      "Epoch 255/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0177\n",
      "Epoch 256/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0179\n",
      "Epoch 257/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0179\n",
      "Epoch 258/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0176\n",
      "Epoch 259/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0176\n",
      "Epoch 260/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.0178\n",
      "Epoch 261/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.0176\n",
      "Epoch 262/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0177\n",
      "Epoch 263/300\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.0166 - val_loss: 0.0175\n",
      "Epoch 264/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0175\n",
      "Epoch 265/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0177\n",
      "Epoch 266/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0175\n",
      "Epoch 267/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0175\n",
      "Epoch 268/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.0174\n",
      "Epoch 269/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0164 - val_loss: 0.0177\n",
      "Epoch 270/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0174\n",
      "Epoch 271/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0164 - val_loss: 0.0174\n",
      "Epoch 272/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0163 - val_loss: 0.0174\n",
      "Epoch 273/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0173\n",
      "Epoch 274/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0178\n",
      "Epoch 275/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0163 - val_loss: 0.0173\n",
      "Epoch 276/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0164 - val_loss: 0.0174\n",
      "Epoch 277/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0174\n",
      "Epoch 278/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0174\n",
      "Epoch 279/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0173\n",
      "Epoch 280/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.0173\n",
      "Epoch 281/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0174\n",
      "Epoch 282/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0172\n",
      "Epoch 283/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0174\n",
      "Epoch 284/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0172\n",
      "Epoch 285/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0175\n",
      "Epoch 286/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0178\n",
      "Epoch 287/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0172\n",
      "Epoch 288/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0171\n",
      "Epoch 289/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0172\n",
      "Epoch 290/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 291/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.0172\n",
      "Epoch 292/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 293/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 294/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0171\n",
      "Epoch 295/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0171\n",
      "Epoch 296/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0171\n",
      "Epoch 297/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0172\n",
      "Epoch 298/300\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0170\n",
      "Epoch 299/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0174\n",
      "Epoch 300/300\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.0157 - val_loss: 0.0172\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0172\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmsle_final_epoch</th>\n",
       "      <th>rmsle_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149032</td>\n",
       "      <td>0.126645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151430</td>\n",
       "      <td>0.122506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.163964</td>\n",
       "      <td>0.104636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.143750</td>\n",
       "      <td>0.126720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.131063</td>\n",
       "      <td>0.125480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rmsle_final_epoch  rmsle_min\n",
       "0           0.149032   0.126645\n",
       "1           0.151430   0.122506\n",
       "2           0.163964   0.104636\n",
       "3           0.143750   0.126720\n",
       "4           0.131063   0.125480"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) (Bonus) Using all your CPU cores to run Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî• **BONUS** üî• **Multiprocessing computing using [dask](https://docs.dask.org/en/latest/delayed.html)** and **all your CPU cores**:\n",
    "\n",
    "_(to mimic SkLearn's `n_jobs=-1`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 11:04:46.326928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 11:04:46.444280: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-29 11:04:46.448063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-29 11:04:46.448090: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-01-29 11:04:46.470098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 11:04:46.968948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-29 11:04:46.969040: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-29 11:04:46.969045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate_model() takes 2 positional arguments but 4 were given\n\nTraceback\n---------\n  File \"/home/parissa/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/parissa/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [388], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m kf \u001b[38;5;241m=\u001b[39m KFold(n_splits \u001b[38;5;241m=\u001b[39m cv, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m f \u001b[38;5;241m=\u001b[39m delayed(evaluate_model)\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocesses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m pd\u001b[38;5;241m.\u001b[39mconcat(results, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/base.py:598\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    596\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 598\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/multiprocessing.py:233\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, num_workers, func_loads, func_dumps, optimize_graph, pool, initializer, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Note former versions used a multiprocessing Manager to share\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# a Queue between parent and workers, but this is fragile on Windows\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# (issue #1652).\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdsk3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_process_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdumps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/multiprocessing.py:111\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(exc, tb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    110\u001b[0m     exc \u001b[38;5;241m=\u001b[39m remote_exception(exc, tb)\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_model() takes 2 positional arguments but 4 were given\n\nTraceback\n---------\n  File \"/home/parissa/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/parissa/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from dask import delayed\n",
    "\n",
    "cv = 5\n",
    "kf = KFold(n_splits = cv, shuffle = True)\n",
    "f = delayed(evaluate_model)\n",
    "\n",
    "results = delayed([f(X, y, train_index, val_index) for (train_index, val_index) in kf.split(X)\n",
    "                  ]).compute(\n",
    "                      scheduler='processes', num_workers=8)\n",
    "\n",
    "pd.concat(results, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (2.4) (Bonus) Multiprocessing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "**multiprocessing with default Python library**\n",
    "\n",
    "References :\n",
    "* [Yitong Ren - Speeding Up and Perfecting Your Work Using Parallel Computing](https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8)\n",
    "* [Johaupt Github - Parallel Processing for Cross Validation - BROKEN LINK](https://johaupt.github.io/python/parallel%20processing/cross-validation/multiprocessing_cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will fail try to debug it yourself if you cannot checkout the hints below\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(processes=2) #mp.cpu_count()-1)\n",
    "\n",
    "results = []\n",
    "def log_result(x):\n",
    "    results.append(x)\n",
    "    \n",
    "for train_index, val_index in kf.split(X):\n",
    "    pool.apply_async(\n",
    "        evaluate_model,\n",
    "        args=(X, y, train_index, val_index),\n",
    "        callback = log_result)\n",
    "\n",
    "# Close the pool for new tasks\n",
    "pool.close()\n",
    "\n",
    "# Wait for all tasks to complete at this point\n",
    "pool.join()\n",
    "\n",
    "result = pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown='span'>Hints</summary>\n",
    "\n",
    "This is a limitation of multiprocessing in ipython enviroments this code would work fine in .py file.\n",
    "The key error is `AttributeError: Can't get attribute 'evaluate_model' on <module 'main' (built-in)>`\n",
    "\n",
    "Checkout this stackoverflow for a workaround https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror !\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## (3) üèÖFINAL SUBMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ü¶Ñ Predict the ***prices of the houses in your test set*** and submit your results to Kaggle! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")\n",
    "# X_test_preproc = preproc.transform(X_test)\n",
    "# ALREADY DONE ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "üíæ Save your predictions in a Dataframe called `results` with the format required by Kaggle so that when you export it to a `.csv`, Kaggle can read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "üì§  Export your results using Kaggle's submission format and submit it online!\n",
    "\n",
    "_(Uncomment the last cell of this notebook)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# results.to_csv(\"submission_final.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "üèÅ Congratulations!\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... it's time for the Recap!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
